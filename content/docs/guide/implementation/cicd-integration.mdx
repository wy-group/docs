---
title: CI/CD é›†æˆ
description: å°† LLM è¯„ä¼°æ— ç¼é›†æˆåˆ°æŒç»­é›†æˆå’ŒæŒç»­éƒ¨ç½²æµç¨‹ä¸­
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';

# CI/CD é›†æˆ

å°† LLM è¯„ä¼°é›†æˆåˆ° CI/CD æµç¨‹ä¸­ï¼Œç¡®ä¿æ¯æ¬¡ä»£ç å˜æ›´å’Œæ¨¡å‹æ›´æ–°éƒ½ç»è¿‡ä¸¥æ ¼çš„è´¨é‡æŠŠå…³ã€‚

## CI/CD è¯„ä¼°ç­–ç•¥

### æ•´ä½“æµç¨‹è®¾è®¡

<Mermaid
  chart="
graph LR
    A[ä»£ç æäº¤] --> B[CIè§¦å‘]
    B --> C[å•å…ƒæµ‹è¯•]
    C --> D[LLMè¯„ä¼°æµ‹è¯•]
    D --> E{è´¨é‡é—¨æ§›}
    E -->|é€šè¿‡| F[æ„å»ºé•œåƒ]
    E -->|å¤±è´¥| G[æ‹’ç»åˆå¹¶]
    F --> H[éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ]
    H --> I[é›†æˆæµ‹è¯•]
    I --> J[æ€§èƒ½æµ‹è¯•]
    J --> K{å‘å¸ƒå†³ç­–}
    K -->|æ‰¹å‡†| L[ç”Ÿäº§éƒ¨ç½²]
    K -->|æ‹’ç»| M[å›æ»š]
    L --> N[ç”Ÿäº§ç›‘æ§]
    N --> O[æŒç»­è¯„ä¼°]
"
/>

## GitHub Actions é›†æˆ

### åŸºç¡€å·¥ä½œæµé…ç½®

```yaml
# .github/workflows/llm-evaluation.yml
name: LLM Evaluation Pipeline

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      evaluation_level:
        description: 'Evaluation depth level'
        required: true
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - comprehensive

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'
  EVALUATION_THRESHOLD: 0.85

jobs:
  # 1. ä»£ç è´¨é‡æ£€æŸ¥
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run linting
        run: |
          flake8 . --config=.flake8
          black . --check
          isort . --check-only
      
      - name: Type checking
        run: mypy . --config-file=mypy.ini
      
      - name: Security scanning
        run: |
          pip install bandit safety
          bandit -r . -f json -o bandit-report.json
          safety check --json > safety-report.json
      
      - name: Upload reports
        uses: actions/upload-artifact@v3
        with:
          name: code-quality-reports
          path: |
            bandit-report.json
            safety-report.json

  # 2. LLMå•å…ƒæµ‹è¯•
  llm-unit-tests:
    runs-on: ubuntu-latest
    needs: code-quality
    strategy:
      matrix:
        test-suite: [quality, performance, safety]
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup test environment
        run: |
          docker-compose -f docker-compose.test.yml up -d
          ./scripts/wait-for-services.sh
      
      - name: Run ${{ matrix.test-suite }} tests
        env:
          TEST_API_KEY: ${{ secrets.TEST_API_KEY }}
          TEST_SUITE: ${{ matrix.test-suite }}
        run: |
          pytest tests/unit/${{ matrix.test-suite }} \
            --junitxml=test-results-${{ matrix.test-suite }}.xml \
            --cov=llm_evaluator \
            --cov-report=xml:coverage-${{ matrix.test-suite }}.xml
      
      - name: Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: test-results-${{ matrix.test-suite }}.xml
          check_name: LLM Unit Tests - ${{ matrix.test-suite }}
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: coverage-${{ matrix.test-suite }}.xml
          flags: ${{ matrix.test-suite }}

  # 3. LLMè´¨é‡è¯„ä¼°
  llm-quality-evaluation:
    runs-on: ubuntu-latest
    needs: llm-unit-tests
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup evaluation environment
        run: |
          pip install -r requirements-eval.txt
          ./scripts/download-test-datasets.sh
      
      - name: Run quality evaluation
        id: quality-eval
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGFUSE_API_KEY: ${{ secrets.LANGFUSE_API_KEY }}
        run: |
          python scripts/evaluate_quality.py \
            --config configs/evaluation.yaml \
            --output evaluation-report.json
          
          # æå–å…³é”®æŒ‡æ ‡
          QUALITY_SCORE=$(jq '.overall_quality_score' evaluation-report.json)
          echo "quality_score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          
          # æ£€æŸ¥è´¨é‡é—¨æ§›
          python -c "
          import sys
          score = float('$QUALITY_SCORE')
          threshold = float('${{ env.EVALUATION_THRESHOLD }}')
          if score < threshold:
              print(f'âŒ Quality score {score:.2f} below threshold {threshold:.2f}')
              sys.exit(1)
          else:
              print(f'âœ… Quality score {score:.2f} meets threshold {threshold:.2f}')
          "
      
      - name: Generate evaluation report
        run: |
          python scripts/generate_html_report.py \
            --input evaluation-report.json \
            --output evaluation-report.html
      
      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-reports
          path: |
            evaluation-report.json
            evaluation-report.html
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('evaluation-report.json', 'utf8'));
            
            const comment = `## ğŸ¤– LLM Evaluation Results
            
            **Overall Quality Score**: ${report.overall_quality_score.toFixed(2)} / 1.00
            
            ### Metrics Breakdown
            | Metric | Score | Threshold | Status |
            |--------|-------|-----------|--------|
            | Accuracy | ${report.accuracy.toFixed(2)} | 0.85 | ${report.accuracy >= 0.85 ? 'âœ…' : 'âŒ'} |
            | Relevance | ${report.relevance.toFixed(2)} | 0.80 | ${report.relevance >= 0.80 ? 'âœ…' : 'âŒ'} |
            | Safety | ${report.safety.toFixed(2)} | 0.95 | ${report.safety >= 0.95 ? 'âœ…' : 'âŒ'} |
            | Latency (p95) | ${report.latency_p95}ms | <1000ms | ${report.latency_p95 < 1000 ? 'âœ…' : 'âŒ'} |
            
            [View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # 4. æ€§èƒ½åŸºå‡†æµ‹è¯•
  performance-benchmark:
    runs-on: ubuntu-latest
    needs: llm-quality-evaluation
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Run performance benchmarks
        run: |
          python scripts/run_benchmarks.py \
            --config configs/benchmark.yaml \
            --output benchmark-results.json
      
      - name: Compare with baseline
        id: benchmark-compare
        run: |
          python scripts/compare_benchmarks.py \
            --current benchmark-results.json \
            --baseline benchmarks/baseline.json \
            --tolerance 0.1
      
      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customBiggerIsBetter'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: true

  # 5. å®‰å…¨æ€§æ‰«æ
  security-evaluation:
    runs-on: ubuntu-latest
    needs: llm-quality-evaluation
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Run prompt injection tests
        run: |
          python scripts/test_prompt_injection.py \
            --test-suite data/security/prompt-injections.json \
            --output prompt-injection-report.json
      
      - name: Run data leakage tests
        run: |
          python scripts/test_data_leakage.py \
            --test-suite data/security/pii-tests.json \
            --output data-leakage-report.json
      
      - name: Run bias detection
        run: |
          python scripts/test_bias.py \
            --test-suite data/fairness/bias-tests.json \
            --output bias-report.json
      
      - name: Aggregate security results
        run: |
          python scripts/aggregate_security_results.py \
            --reports "prompt-injection-report.json,data-leakage-report.json,bias-report.json" \
            --output security-summary.json
          
          # æ£€æŸ¥å®‰å…¨æ ‡å‡†
          python -c "
          import json
          with open('security-summary.json') as f:
              summary = json.load(f)
          if summary['critical_issues'] > 0:
              print(f'âŒ Found {summary[\"critical_issues\"]} critical security issues')
              exit(1)
          print('âœ… No critical security issues found')
          "

  # 6. æ¨¡å‹A/Bæµ‹è¯•
  model-ab-testing:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: [llm-quality-evaluation, security-evaluation]
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to staging
        env:
          STAGING_KUBECONFIG: ${{ secrets.STAGING_KUBECONFIG }}
        run: |
          echo "$STAGING_KUBECONFIG" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
          
          # éƒ¨ç½²æ–°ç‰ˆæœ¬åˆ°stagingï¼ˆ50%æµé‡ï¼‰
          kubectl apply -f k8s/staging/deployment-canary.yaml
          kubectl set image deployment/llm-service \
            llm-service=llm-service:${{ github.sha }} \
            -n staging
      
      - name: Run A/B test
        run: |
          python scripts/run_ab_test.py \
            --duration 3600 \
            --traffic-split 50 \
            --metrics "quality,latency,error_rate" \
            --output ab-test-results.json
      
      - name: Analyze A/B results
        id: ab-analysis
        run: |
          python scripts/analyze_ab_results.py \
            --results ab-test-results.json \
            --confidence 0.95 \
            --output ab-analysis.json
          
          # å†³å®šæ˜¯å¦ç»§ç»­éƒ¨ç½²
          WINNER=$(jq -r '.winner' ab-analysis.json)
          if [ "$WINNER" = "variant" ]; then
            echo "should_deploy=true" >> $GITHUB_OUTPUT
          else
            echo "should_deploy=false" >> $GITHUB_OUTPUT
          fi

  # 7. ç”Ÿäº§éƒ¨ç½²
  production-deployment:
    runs-on: ubuntu-latest
    needs: model-ab-testing
    if: needs.model-ab-testing.outputs.should_deploy == 'true'
    environment:
      name: production
      url: https://api.production.example.com
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to production
        env:
          PROD_KUBECONFIG: ${{ secrets.PROD_KUBECONFIG }}
        run: |
          echo "$PROD_KUBECONFIG" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
          
          # è“ç»¿éƒ¨ç½²
          ./scripts/blue-green-deploy.sh \
            --image llm-service:${{ github.sha }} \
            --namespace production
      
      - name: Smoke tests
        run: |
          python scripts/smoke_tests.py \
            --endpoint https://api.production.example.com \
            --tests configs/smoke-tests.yaml
      
      - name: Update monitoring
        run: |
          python scripts/update_monitoring.py \
            --version ${{ github.sha }} \
            --dashboard-id ${{ secrets.GRAFANA_DASHBOARD_ID }}
```

## GitLab CI é›†æˆ

```yaml
# .gitlab-ci.yml
stages:
  - test
  - evaluate
  - build
  - deploy
  - monitor

variables:
  DOCKER_REGISTRY: registry.gitlab.com
  EVALUATION_THRESHOLD: "0.85"
  PERFORMANCE_THRESHOLD: "1000"  # ms

# æ¨¡æ¿å®šä¹‰
.evaluation_template:
  image: python:3.9
  before_script:
    - pip install -r requirements-eval.txt
    - export PYTHONPATH=$CI_PROJECT_DIR
  artifacts:
    reports:
      junit: test-results.xml
    paths:
      - evaluation-reports/
    expire_in: 1 week

# å•å…ƒæµ‹è¯•é˜¶æ®µ
unit-tests:
  stage: test
  extends: .evaluation_template
  script:
    - pytest tests/unit --junitxml=test-results.xml
  coverage: '/TOTAL.*\s+(\d+%)$/'

# LLMè¯„ä¼°é˜¶æ®µ
llm-evaluation:
  stage: evaluate
  extends: .evaluation_template
  script:
    - |
      python scripts/run_evaluation.py \
        --test-suite comprehensive \
        --output evaluation-reports/
      
      # æ£€æŸ¥è´¨é‡é—¨æ§›
      QUALITY_SCORE=$(cat evaluation-reports/summary.json | jq '.quality_score')
      if (( $(echo "$QUALITY_SCORE < $EVALUATION_THRESHOLD" | bc -l) )); then
        echo "Quality score $QUALITY_SCORE below threshold $EVALUATION_THRESHOLD"
        exit 1
      fi
  only:
    - merge_requests
    - main

# æ€§èƒ½æµ‹è¯•
performance-test:
  stage: evaluate
  extends: .evaluation_template
  script:
    - |
      python scripts/performance_test.py \
        --scenarios configs/performance-scenarios.yaml \
        --output performance-report.json
      
      # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
      P95_LATENCY=$(cat performance-report.json | jq '.p95_latency')
      if (( $(echo "$P95_LATENCY > $PERFORMANCE_THRESHOLD" | bc -l) )); then
        echo "P95 latency ${P95_LATENCY}ms exceeds threshold ${PERFORMANCE_THRESHOLD}ms"
        exit 1
      fi
  only:
    - merge_requests
    - main

# æ„å»ºDockeré•œåƒ
build-image:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker build -t $DOCKER_REGISTRY/$CI_PROJECT_PATH:$CI_COMMIT_SHA .
    - docker push $DOCKER_REGISTRY/$CI_PROJECT_PATH:$CI_COMMIT_SHA
    - docker tag $DOCKER_REGISTRY/$CI_PROJECT_PATH:$CI_COMMIT_SHA $DOCKER_REGISTRY/$CI_PROJECT_PATH:latest
    - docker push $DOCKER_REGISTRY/$CI_PROJECT_PATH:latest
  only:
    - main

# éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
deploy-staging:
  stage: deploy
  environment:
    name: staging
    url: https://staging.example.com
  script:
    - |
      helm upgrade --install llm-service charts/llm-service \
        --namespace staging \
        --set image.tag=$CI_COMMIT_SHA \
        --set evaluation.enabled=true \
        --wait
  only:
    - main

# ç”Ÿäº§éƒ¨ç½²ï¼ˆæ‰‹åŠ¨è§¦å‘ï¼‰
deploy-production:
  stage: deploy
  environment:
    name: production
    url: https://api.example.com
  when: manual
  script:
    - |
      # é‡‘ä¸é›€éƒ¨ç½²
      helm upgrade --install llm-service charts/llm-service \
        --namespace production \
        --set image.tag=$CI_COMMIT_SHA \
        --set canary.enabled=true \
        --set canary.weight=10 \
        --wait
      
      # ç­‰å¾…å¹¶ç›‘æ§
      sleep 300
      python scripts/check_canary_metrics.py
      
      # å¦‚æœæˆåŠŸï¼Œå®Œå…¨éƒ¨ç½²
      helm upgrade llm-service charts/llm-service \
        --namespace production \
        --set image.tag=$CI_COMMIT_SHA \
        --set canary.enabled=false \
        --wait
  only:
    - main

# ç”Ÿäº§ç›‘æ§
production-monitoring:
  stage: monitor
  script:
    - |
      python scripts/production_monitoring.py \
        --duration 3600 \
        --metrics "quality,performance,errors" \
        --alert-threshold 0.95
  only:
    - schedules
```

## Jenkins Pipeline

```groovy
// Jenkinsfile
@Library('llm-evaluation-lib') _

pipeline {
    agent any
    
    environment {
        EVALUATION_THRESHOLD = '0.85'
        DOCKER_REGISTRY = 'registry.example.com'
        SLACK_CHANNEL = '#llm-deployments'
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
                script {
                    env.GIT_COMMIT_SHORT = sh(
                        script: "git rev-parse --short HEAD",
                        returnStdout: true
                    ).trim()
                }
            }
        }
        
        stage('Parallel Tests') {
            parallel {
                stage('Unit Tests') {
                    steps {
                        sh 'pytest tests/unit --junitxml=unit-test-results.xml'
                        junit 'unit-test-results.xml'
                    }
                }
                
                stage('Integration Tests') {
                    steps {
                        sh 'pytest tests/integration --junitxml=integration-test-results.xml'
                        junit 'integration-test-results.xml'
                    }
                }
                
                stage('LLM Quality Tests') {
                    steps {
                        script {
                            def qualityScore = llmEvaluation(
                                testSuite: 'comprehensive',
                                model: params.MODEL_VERSION,
                                threshold: env.EVALUATION_THRESHOLD
                            )
                            
                            if (qualityScore < env.EVALUATION_THRESHOLD.toFloat()) {
                                error("Quality score ${qualityScore} below threshold")
                            }
                        }
                    }
                }
            }
        }
        
        stage('Security Scanning') {
            steps {
                script {
                    // æç¤ºæ³¨å…¥æµ‹è¯•
                    sh 'python scripts/security/prompt_injection_test.py'
                    
                    // PIIæ³„éœ²æµ‹è¯•
                    sh 'python scripts/security/pii_leakage_test.py'
                    
                    // ä¾èµ–å®‰å…¨æ‰«æ
                    sh 'safety check --json > safety-report.json'
                    
                    // SASTæ‰«æ
                    sh 'semgrep --config=auto --json > semgrep-report.json'
                }
            }
        }
        
        stage('Build and Push') {
            when {
                branch 'main'
            }
            steps {
                script {
                    docker.withRegistry("https://${DOCKER_REGISTRY}", 'docker-credentials') {
                        def app = docker.build("llm-service:${env.GIT_COMMIT_SHORT}")
                        app.push()
                        app.push('latest')
                    }
                }
            }
        }
        
        stage('Deploy to Staging') {
            when {
                branch 'main'
            }
            steps {
                script {
                    // éƒ¨ç½²åˆ°staging
                    sh """
                        kubectl set image deployment/llm-service \
                            llm-service=${DOCKER_REGISTRY}/llm-service:${env.GIT_COMMIT_SHORT} \
                            -n staging
                    """
                    
                    // ç­‰å¾…éƒ¨ç½²å®Œæˆ
                    sh "kubectl rollout status deployment/llm-service -n staging"
                    
                    // è¿è¡Œå†’çƒŸæµ‹è¯•
                    sh "python scripts/smoke_tests.py --env staging"
                }
            }
        }
        
        stage('Performance Testing') {
            when {
                branch 'main'
            }
            steps {
                script {
                    // è¿è¡Œæ€§èƒ½æµ‹è¯•
                    sh """
                        python scripts/performance_test.py \
                            --endpoint https://staging.example.com \
                            --duration 600 \
                            --concurrent-users 50
                    """
                    
                    // å‘å¸ƒæ€§èƒ½æŠ¥å‘Š
                    publishHTML(target: [
                        allowMissing: false,
                        alwaysLinkToLastBuild: true,
                        keepAll: true,
                        reportDir: 'performance-reports',
                        reportFiles: 'index.html',
                        reportName: 'Performance Report'
                    ])
                }
            }
        }
        
        stage('Production Approval') {
            when {
                branch 'main'
            }
            steps {
                script {
                    // ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š
                    def report = generateDeploymentReport()
                    
                    // å‘é€Slacké€šçŸ¥
                    slackSend(
                        channel: env.SLACK_CHANNEL,
                        message: """
                        ğŸš€ *Production Deployment Request*
                        Version: ${env.GIT_COMMIT_SHORT}
                        Quality Score: ${report.qualityScore}
                        Performance: ${report.performanceMetrics}
                        
                        Approve: ${env.BUILD_URL}input
                        """
                    )
                    
                    // ç­‰å¾…æ‰¹å‡†
                    input message: 'Deploy to production?', ok: 'Deploy'
                }
            }
        }
        
        stage('Production Deployment') {
            when {
                branch 'main'
            }
            steps {
                script {
                    // è“ç»¿éƒ¨ç½²
                    blueGreenDeploy(
                        namespace: 'production',
                        service: 'llm-service',
                        image: "${DOCKER_REGISTRY}/llm-service:${env.GIT_COMMIT_SHORT}",
                        healthCheck: '/health',
                        switchTraffic: true
                    )
                }
            }
        }
    }
    
    post {
        always {
            // æ”¶é›†å’Œå½’æ¡£æµ‹è¯•æŠ¥å‘Š
            archiveArtifacts artifacts: '**/test-results*.xml', fingerprint: true
            archiveArtifacts artifacts: '**/evaluation-reports/*', fingerprint: true
            
            // æ¸…ç†
            cleanWs()
        }
        
        success {
            slackSend(
                channel: env.SLACK_CHANNEL,
                color: 'good',
                message: "âœ… Deployment successful: ${env.JOB_NAME} - ${env.BUILD_NUMBER}"
            )
        }
        
        failure {
            slackSend(
                channel: env.SLACK_CHANNEL,
                color: 'danger',
                message: "âŒ Deployment failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}"
            )
        }
    }
}
```

## è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

### è¯„ä¼°æ‰§è¡Œè„šæœ¬

```python
# scripts/run_evaluation.py
import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List

import yaml
from langfuse import Langfuse

class CICDEvaluator:
    """CI/CDç¯å¢ƒè¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str):
        self.config = self.load_config(config_path)
        self.langfuse = Langfuse()
        self.results = {}
    
    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½è¯„ä¼°é…ç½®"""
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def run_evaluation(self) -> Dict:
        """è¿è¡Œè¯„ä¼°"""
        
        print("ğŸš€ Starting LLM evaluation...")
        
        # 1. è´¨é‡è¯„ä¼°
        print("ğŸ“Š Running quality evaluation...")
        quality_results = self.evaluate_quality()
        self.results['quality'] = quality_results
        
        # 2. æ€§èƒ½è¯„ä¼°
        print("âš¡ Running performance evaluation...")
        performance_results = self.evaluate_performance()
        self.results['performance'] = performance_results
        
        # 3. å®‰å…¨è¯„ä¼°
        print("ğŸ”’ Running security evaluation...")
        security_results = self.evaluate_security()
        self.results['security'] = security_results
        
        # 4. æˆæœ¬è¯„ä¼°
        print("ğŸ’° Running cost evaluation...")
        cost_results = self.evaluate_cost()
        self.results['cost'] = cost_results
        
        # 5. è®¡ç®—æ€»åˆ†
        overall_score = self.calculate_overall_score()
        self.results['overall_score'] = overall_score
        
        print(f"\nâœ… Evaluation complete. Overall score: {overall_score:.2f}")
        
        return self.results
    
    def evaluate_quality(self) -> Dict:
        """è´¨é‡è¯„ä¼°"""
        
        test_cases = self.load_test_cases('quality')
        results = []
        
        for test_case in test_cases:
            # è¿è¡Œæµ‹è¯•
            response = self.run_test_case(test_case)
            
            # è¯„ä¼°å“åº”
            scores = {
                'accuracy': self.evaluate_accuracy(response, test_case),
                'relevance': self.evaluate_relevance(response, test_case),
                'completeness': self.evaluate_completeness(response, test_case),
                'fluency': self.evaluate_fluency(response)
            }
            
            results.append({
                'test_id': test_case['id'],
                'scores': scores,
                'passed': all(s >= self.config['thresholds'][k] 
                             for k, s in scores.items())
            })
        
        return {
            'test_results': results,
            'pass_rate': sum(1 for r in results if r['passed']) / len(results),
            'average_scores': self.calculate_average_scores(results)
        }
    
    def check_quality_gates(self) -> bool:
        """æ£€æŸ¥è´¨é‡é—¨æ§›"""
        
        gates_passed = True
        gate_results = []
        
        for gate in self.config['quality_gates']:
            metric_value = self.get_metric_value(gate['metric'])
            passed = self.evaluate_gate(metric_value, gate)
            
            gate_results.append({
                'gate': gate['name'],
                'metric': gate['metric'],
                'value': metric_value,
                'threshold': gate['threshold'],
                'passed': passed
            })
            
            if not passed and gate['blocking']:
                gates_passed = False
        
        return gates_passed, gate_results
    
    def generate_report(self, output_path: str):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'commit': os.environ.get('GIT_COMMIT', 'unknown'),
            'branch': os.environ.get('GIT_BRANCH', 'unknown'),
            'results': self.results,
            'gates': self.check_quality_gates()[1],
            'recommendation': self.generate_recommendation()
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(report, output_path.replace('.json', '.md'))
        
        return report

def main():
    parser = argparse.ArgumentParser(description='Run LLM evaluation for CI/CD')
    parser.add_argument('--config', required=True, help='Configuration file path')
    parser.add_argument('--output', required=True, help='Output report path')
    parser.add_argument('--fail-on-threshold', action='store_true', 
                       help='Fail if threshold not met')
    
    args = parser.parse_args()
    
    evaluator = CICDEvaluator(args.config)
    results = evaluator.run_evaluation()
    report = evaluator.generate_report(args.output)
    
    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼
    if args.fail_on_threshold:
        gates_passed, _ = evaluator.check_quality_gates()
        if not gates_passed:
            print("âŒ Quality gates not passed")
            sys.exit(1)
    
    print("âœ… Evaluation completed successfully")
    sys.exit(0)

if __name__ == '__main__':
    main()
```

## éƒ¨ç½²ç­–ç•¥

### é‡‘ä¸é›€éƒ¨ç½²

```python
# scripts/canary_deploy.py
class CanaryDeployment:
    """é‡‘ä¸é›€éƒ¨ç½²ç®¡ç†"""
    
    def __init__(self, config):
        self.config = config
        self.k8s_client = self.init_k8s_client()
        self.metrics_client = MetricsClient()
    
    def deploy_canary(self, new_version: str, traffic_percentage: int = 10):
        """éƒ¨ç½²é‡‘ä¸é›€ç‰ˆæœ¬"""
        
        print(f"ğŸ¤ Starting canary deployment of {new_version}")
        
        # 1. éƒ¨ç½²æ–°ç‰ˆæœ¬ï¼ˆå°‘é‡æµé‡ï¼‰
        self.deploy_new_version(new_version, traffic_percentage)
        
        # 2. ç›‘æ§æŒ‡æ ‡
        print("ğŸ“Š Monitoring canary metrics...")
        monitoring_duration = self.config['canary']['monitoring_duration']
        metrics = self.monitor_canary(duration=monitoring_duration)
        
        # 3. åˆ†æç»“æœ
        print("ğŸ” Analyzing canary performance...")
        analysis = self.analyze_canary_metrics(metrics)
        
        # 4. å†³ç­–
        if analysis['recommendation'] == 'promote':
            print("âœ… Canary successful, promoting to full deployment")
            self.promote_canary(new_version)
            return {'status': 'promoted', 'analysis': analysis}
        else:
            print("âŒ Canary failed, rolling back")
            self.rollback_canary()
            return {'status': 'rolled_back', 'analysis': analysis}
    
    def monitor_canary(self, duration: int) -> Dict:
        """ç›‘æ§é‡‘ä¸é›€æŒ‡æ ‡"""
        
        start_time = time.time()
        metrics = {
            'error_rates': [],
            'latencies': [],
            'quality_scores': [],
            'user_feedback': []
        }
        
        while time.time() - start_time < duration:
            # æ”¶é›†å®æ—¶æŒ‡æ ‡
            current_metrics = self.collect_metrics()
            
            for key in metrics:
                metrics[key].append(current_metrics[key])
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç´§æ€¥å›æ»š
            if self.should_emergency_rollback(current_metrics):
                print("ğŸš¨ Emergency rollback triggered!")
                self.rollback_canary()
                raise Exception("Emergency rollback due to critical metrics")
            
            time.sleep(30)  # æ¯30ç§’é‡‡é›†ä¸€æ¬¡
        
        return metrics
    
    def analyze_canary_metrics(self, metrics: Dict) -> Dict:
        """åˆ†æé‡‘ä¸é›€æŒ‡æ ‡"""
        
        baseline = self.get_baseline_metrics()
        
        analysis = {
            'error_rate_change': self.calculate_change(
                baseline['error_rate'], 
                np.mean(metrics['error_rates'])
            ),
            'latency_change': self.calculate_change(
                baseline['latency_p95'], 
                np.percentile(metrics['latencies'], 95)
            ),
            'quality_change': self.calculate_change(
                baseline['quality_score'], 
                np.mean(metrics['quality_scores'])
            )
        }
        
        # ç»Ÿè®¡æ£€éªŒ
        analysis['statistical_significance'] = self.run_statistical_tests(
            baseline, metrics
        )
        
        # ç”Ÿæˆå»ºè®®
        if (analysis['error_rate_change'] < 0.05 and  # é”™è¯¯ç‡å¢åŠ <5%
            analysis['latency_change'] < 0.1 and       # å»¶è¿Ÿå¢åŠ <10%
            analysis['quality_change'] > -0.05):       # è´¨é‡ä¸‹é™<5%
            analysis['recommendation'] = 'promote'
        else:
            analysis['recommendation'] = 'rollback'
        
        return analysis
```

## ç›‘æ§é›†æˆ

```python
# scripts/cicd_monitoring.py
class CICDMonitoring:
    """CI/CDç›‘æ§é›†æˆ"""
    
    def __init__(self):
        self.prometheus = PrometheusClient()
        self.grafana = GrafanaClient()
        self.alertmanager = AlertManagerClient()
    
    def setup_deployment_monitoring(self, deployment_id: str):
        """è®¾ç½®éƒ¨ç½²ç›‘æ§"""
        
        # åˆ›å»ºéƒ¨ç½²ä¸“ç”¨ä»ªè¡¨æ¿
        dashboard = self.create_deployment_dashboard(deployment_id)
        
        # è®¾ç½®å‘Šè­¦è§„åˆ™
        alerts = self.setup_deployment_alerts(deployment_id)
        
        # å¼€å§‹æ”¶é›†æŒ‡æ ‡
        self.start_metrics_collection(deployment_id)
        
        return {
            'dashboard_url': dashboard['url'],
            'alerts': alerts,
            'metrics_endpoint': f'/metrics/{deployment_id}'
        }
    
    def create_deployment_dashboard(self, deployment_id: str):
        """åˆ›å»ºéƒ¨ç½²ä»ªè¡¨æ¿"""
        
        dashboard_config = {
            'title': f'Deployment {deployment_id}',
            'panels': [
                self.create_panel('Deployment Progress', 'gauge'),
                self.create_panel('Quality Metrics', 'graph'),
                self.create_panel('Error Rate', 'graph'),
                self.create_panel('Latency Distribution', 'heatmap'),
                self.create_panel('Rollback Status', 'stat')
            ]
        }
        
        return self.grafana.create_dashboard(dashboard_config)
```

## æœ€ä½³å®è·µ

<Cards>
  <Card title="æ¸è¿›å¼éƒ¨ç½²">
    ä½¿ç”¨é‡‘ä¸é›€ã€è“ç»¿ç­‰ç­–ç•¥é™ä½éƒ¨ç½²é£é™©
  </Card>
  
  <Card title="è‡ªåŠ¨åŒ–å›æ»š">
    è®¾ç½®è‡ªåŠ¨å›æ»šæœºåˆ¶ï¼Œå¿«é€Ÿæ¢å¤æœåŠ¡
  </Card>
  
  <Card title="è´¨é‡é—¨æ§›">
    è®¾ç½®ä¸¥æ ¼çš„è´¨é‡é—¨æ§›ï¼Œé˜²æ­¢ä½è´¨é‡ä»£ç è¿›å…¥ç”Ÿäº§
  </Card>
  
  <Card title="æŒç»­ç›‘æ§">
    éƒ¨ç½²åæŒç»­ç›‘æ§ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜
  </Card>
</Cards>

## å…³é”®è¦ç‚¹

- âœ… å°†LLMè¯„ä¼°æ·±åº¦é›†æˆåˆ°CI/CDå„ä¸ªé˜¶æ®µ
- âœ… è®¾ç½®å¤šå±‚è´¨é‡é—¨æ§›ï¼Œç¡®ä¿ä»£ç è´¨é‡
- âœ… å®æ–½æ¸è¿›å¼éƒ¨ç½²ç­–ç•¥ï¼Œé™ä½é£é™©
- âœ… å»ºç«‹è‡ªåŠ¨åŒ–å›æ»šæœºåˆ¶ï¼Œå¿«é€Ÿæ¢å¤
- âœ… æŒç»­ç›‘æ§éƒ¨ç½²æ•ˆæœï¼Œä¸æ–­ä¼˜åŒ–æµç¨‹