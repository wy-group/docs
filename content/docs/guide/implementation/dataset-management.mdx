---
title: æ•°æ®é›†ç®¡ç†
description: æ„å»ºå’Œç®¡ç†é«˜è´¨é‡çš„ LLM è¯„ä¼°æ•°æ®é›†ä½“ç³»
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';

# æ•°æ®é›†ç®¡ç†

é«˜è´¨é‡çš„è¯„ä¼°æ•°æ®é›†æ˜¯ LLM ç³»ç»Ÿè¯„ä¼°çš„åŸºç¡€ã€‚æœ¬ç« ä»‹ç»å¦‚ä½•æ„å»ºã€ç®¡ç†å’Œç»´æŠ¤è¯„ä¼°æ•°æ®é›†ã€‚

## æ•°æ®é›†æ¶æ„è®¾è®¡

### æ•°æ®é›†å±‚æ¬¡ç»“æ„

<Mermaid
  chart="
graph TD
    A[æ•°æ®é›†ä½“ç³»] --> B[é»„é‡‘æ•°æ®é›†]
    A --> C[æµ‹è¯•æ•°æ®é›†]
    A --> D[åŸºå‡†æ•°æ®é›†]
    A --> E[ç”Ÿäº§æ•°æ®é›†]
    
    B --> B1[äººå·¥æ ‡æ³¨]
    B --> B2[ä¸“å®¶éªŒè¯]
    B --> B3[ç‰ˆæœ¬æ§åˆ¶]
    
    C --> C1[å•å…ƒæµ‹è¯•]
    C --> C2[é›†æˆæµ‹è¯•]
    C --> C3[å›å½’æµ‹è¯•]
    
    D --> D1[è¡Œä¸šåŸºå‡†]
    D --> D2[å­¦æœ¯åŸºå‡†]
    D --> D3[å†…éƒ¨åŸºå‡†]
    
    E --> E1[å®æ—¶é‡‡æ ·]
    E --> E2[ç”¨æˆ·åé¦ˆ]
    E --> E3[è¾¹ç¼˜æ¡ˆä¾‹]
"
/>

## æ•°æ®é›†æ„å»º

### 1. æ•°æ®æ”¶é›†ç­–ç•¥

```python
class DatasetBuilder:
    """æ•°æ®é›†æ„å»ºå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.data_sources = self.init_data_sources()
        self.quality_checker = DataQualityChecker()
        self.storage = DatasetStorage()
    
    def collect_data(self, source_type: str, **kwargs):
        """æ”¶é›†æ•°æ®"""
        
        collectors = {
            'production': self.collect_from_production,
            'synthetic': self.generate_synthetic_data,
            'crowdsourcing': self.collect_from_crowdsourcing,
            'expert': self.collect_from_experts,
            'public': self.collect_from_public_datasets
        }
        
        if source_type not in collectors:
            raise ValueError(f"Unknown source type: {source_type}")
        
        return collectors[source_type](**kwargs)
    
    def collect_from_production(self, 
                               start_date: datetime,
                               end_date: datetime,
                               sampling_rate: float = 0.01):
        """ä»ç”Ÿäº§ç¯å¢ƒæ”¶é›†æ•°æ®"""
        
        print(f"ğŸ“Š Collecting production data from {start_date} to {end_date}")
        
        # è¿æ¥ç”Ÿäº§æ•°æ®æº
        prod_client = self.data_sources['production']
        
        # æŸ¥è¯¢æ•°æ®
        raw_data = prod_client.query(
            """
            SELECT 
                request_id,
                timestamp,
                user_id,
                input_text,
                output_text,
                model_version,
                parameters,
                metrics,
                user_feedback
            FROM llm_requests
            WHERE timestamp BETWEEN %s AND %s
            AND random() < %s
            """,
            (start_date, end_date, sampling_rate)
        )
        
        # æ•°æ®æ¸…æ´—
        cleaned_data = []
        for record in raw_data:
            # ç§»é™¤PII
            record = self.remove_pii(record)
            
            # è´¨é‡æ£€æŸ¥
            if self.quality_checker.check(record):
                cleaned_data.append(record)
        
        print(f"âœ… Collected {len(cleaned_data)} samples from production")
        
        return cleaned_data
    
    def generate_synthetic_data(self, 
                               template_file: str,
                               num_samples: int,
                               diversity_level: float = 0.7):
        """ç”Ÿæˆåˆæˆæ•°æ®"""
        
        print(f"ğŸ¤– Generating {num_samples} synthetic samples")
        
        # åŠ è½½æ¨¡æ¿
        templates = self.load_templates(template_file)
        
        synthetic_data = []
        
        for i in range(num_samples):
            # é€‰æ‹©æ¨¡æ¿
            template = self.select_template(templates, diversity_level)
            
            # ç”Ÿæˆå˜ä½“
            sample = self.generate_variation(template)
            
            # è‡ªåŠ¨æ ‡æ³¨
            sample['expected_output'] = self.auto_label(sample['input'])
            
            # æ·»åŠ å…ƒæ•°æ®
            sample['metadata'] = {
                'source': 'synthetic',
                'template_id': template['id'],
                'generation_date': datetime.now(),
                'diversity_score': self.calculate_diversity(sample, synthetic_data)
            }
            
            synthetic_data.append(sample)
        
        return synthetic_data
    
    def augment_dataset(self, base_dataset: List[Dict]) -> List[Dict]:
        """æ•°æ®å¢å¼º"""
        
        augmented = []
        
        augmentation_strategies = [
            self.paraphrase_augmentation,
            self.back_translation_augmentation,
            self.noise_injection_augmentation,
            self.context_modification_augmentation
        ]
        
        for sample in base_dataset:
            # åŸå§‹æ ·æœ¬
            augmented.append(sample)
            
            # åº”ç”¨å¢å¼ºç­–ç•¥
            for strategy in augmentation_strategies:
                if random.random() < self.config['augmentation_probability']:
                    augmented_sample = strategy(sample)
                    augmented_sample['metadata']['augmentation'] = strategy.__name__
                    augmented.append(augmented_sample)
        
        return augmented
    
    def balance_dataset(self, dataset: List[Dict]) -> List[Dict]:
        """å¹³è¡¡æ•°æ®é›†"""
        
        # åˆ†æç±»åˆ«åˆ†å¸ƒ
        category_distribution = self.analyze_distribution(dataset)
        
        # ç¡®å®šç›®æ ‡åˆ†å¸ƒ
        target_distribution = self.config.get('target_distribution', 'uniform')
        
        if target_distribution == 'uniform':
            # å‡åŒ€åˆ†å¸ƒ
            min_count = min(category_distribution.values())
            balanced = []
            
            for category, samples in self.group_by_category(dataset).items():
                balanced.extend(random.sample(samples, min_count))
        
        elif target_distribution == 'stratified':
            # åˆ†å±‚é‡‡æ ·
            balanced = self.stratified_sampling(dataset, category_distribution)
        
        else:
            # è‡ªå®šä¹‰åˆ†å¸ƒ
            balanced = self.custom_distribution_sampling(
                dataset, 
                target_distribution
            )
        
        return balanced
```

### 2. æ•°æ®æ ‡æ³¨ç®¡ç†

<Tabs items={['æ ‡æ³¨æµç¨‹', 'è´¨é‡æ§åˆ¶', 'æ ‡æ³¨å·¥å…·', 'æˆæœ¬ä¼˜åŒ–']}>
  <Tab value="æ ‡æ³¨æµç¨‹">
    ```python
    class AnnotationManager:
        """æ ‡æ³¨ç®¡ç†å™¨"""
        
        def __init__(self):
            self.annotators = []
            self.annotation_queue = Queue()
            self.quality_controller = AnnotationQualityController()
        
        def create_annotation_task(self, dataset: List[Dict], 
                                  task_config: Dict) -> str:
            """åˆ›å»ºæ ‡æ³¨ä»»åŠ¡"""
            
            task = {
                'id': str(uuid.uuid4()),
                'created_at': datetime.now(),
                'dataset': dataset,
                'config': task_config,
                'status': 'pending',
                'annotations': {},
                'quality_metrics': {}
            }
            
            # åˆ†é…æ ‡æ³¨è€…
            assignments = self.assign_annotators(task)
            
            # åˆ›å»ºæ ‡æ³¨ç•Œé¢
            interface_url = self.create_annotation_interface(task)
            
            # å‘é€ä»»åŠ¡
            for annotator_id, samples in assignments.items():
                self.send_task_to_annotator(
                    annotator_id,
                    task['id'],
                    samples,
                    interface_url
                )
            
            return task['id']
        
        def assign_annotators(self, task: Dict) -> Dict:
            """åˆ†é…æ ‡æ³¨è€…"""
            
            assignments = {}
            samples = task['dataset']
            
            # æ ¹æ®æ ‡æ³¨è€…ä¸“é•¿åˆ†é…
            for sample in samples:
                best_annotator = self.find_best_annotator(
                    sample,
                    task['config']['required_expertise']
                )
                
                if best_annotator not in assignments:
                    assignments[best_annotator] = []
                
                assignments[best_annotator].append(sample)
            
            # æ·»åŠ å†—ä½™æ ‡æ³¨ï¼ˆç”¨äºä¸€è‡´æ€§æ£€æŸ¥ï¼‰
            if task['config'].get('redundancy', 0) > 1:
                assignments = self.add_redundant_annotations(
                    assignments,
                    task['config']['redundancy']
                )
            
            return assignments
        
        def process_annotation(self, annotation: Dict):
            """å¤„ç†æ ‡æ³¨ç»“æœ"""
            
            # éªŒè¯æ ‡æ³¨
            validation_result = self.validate_annotation(annotation)
            
            if not validation_result['valid']:
                return self.request_reannotation(
                    annotation,
                    validation_result['reasons']
                )
            
            # è´¨é‡è¯„åˆ†
            quality_score = self.quality_controller.score(annotation)
            
            # å­˜å‚¨æ ‡æ³¨
            self.store_annotation(annotation, quality_score)
            
            # æ›´æ–°æ ‡æ³¨è€…ç»Ÿè®¡
            self.update_annotator_stats(
                annotation['annotator_id'],
                quality_score
            )
            
            # æ£€æŸ¥ä»»åŠ¡å®Œæˆåº¦
            if self.is_task_complete(annotation['task_id']):
                self.finalize_task(annotation['task_id'])
    ```
  </Tab>
  
  <Tab value="è´¨é‡æ§åˆ¶">
    ```python
    class AnnotationQualityController:
        """æ ‡æ³¨è´¨é‡æ§åˆ¶"""
        
        def __init__(self):
            self.gold_standards = self.load_gold_standards()
            self.agreement_calculator = InterRaterAgreement()
        
        def implement_quality_measures(self, task_id: str):
            """å®æ–½è´¨é‡æªæ–½"""
            
            measures = {
                'gold_standard_check': self.insert_gold_standards(task_id),
                'duplicate_check': self.insert_duplicates(task_id),
                'consistency_check': self.check_consistency(task_id),
                'expert_review': self.schedule_expert_review(task_id)
            }
            
            return measures
        
        def insert_gold_standards(self, task_id: str, percentage: float = 0.1):
            """æ’å…¥é»„é‡‘æ ‡å‡†æ ·æœ¬"""
            
            task = self.get_task(task_id)
            dataset = task['dataset']
            
            # è®¡ç®—éœ€è¦æ’å…¥çš„æ•°é‡
            num_gold = int(len(dataset) * percentage)
            
            # éšæœºé€‰æ‹©é»„é‡‘æ ‡å‡†
            gold_samples = random.sample(self.gold_standards, num_gold)
            
            # éšæœºæ’å…¥åˆ°æ•°æ®é›†ä¸­
            for gold in gold_samples:
                insert_position = random.randint(0, len(dataset))
                gold['is_gold_standard'] = True
                gold['expected_annotation'] = gold['annotation']
                dataset.insert(insert_position, gold)
            
            return num_gold
        
        def calculate_inter_annotator_agreement(self, annotations: List[Dict]):
            """è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§"""
            
            # åˆ†ç»„ç›¸åŒæ ·æœ¬çš„ä¸åŒæ ‡æ³¨
            grouped = defaultdict(list)
            for ann in annotations:
                grouped[ann['sample_id']].append(ann)
            
            # è®¡ç®—å„ç§ä¸€è‡´æ€§æŒ‡æ ‡
            metrics = {
                'cohen_kappa': [],
                'fleiss_kappa': [],
                'krippendorff_alpha': [],
                'percentage_agreement': []
            }
            
            for sample_id, sample_annotations in grouped.items():
                if len(sample_annotations) >= 2:
                    # Cohen's Kappa (ä¸¤ä¸ªæ ‡æ³¨è€…)
                    if len(sample_annotations) == 2:
                        kappa = self.agreement_calculator.cohen_kappa(
                            sample_annotations[0]['labels'],
                            sample_annotations[1]['labels']
                        )
                        metrics['cohen_kappa'].append(kappa)
                    
                    # Fleiss' Kappa (å¤šä¸ªæ ‡æ³¨è€…)
                    if len(sample_annotations) > 2:
                        kappa = self.agreement_calculator.fleiss_kappa(
                            [ann['labels'] for ann in sample_annotations]
                        )
                        metrics['fleiss_kappa'].append(kappa)
                    
                    # Krippendorff's Alpha
                    alpha = self.agreement_calculator.krippendorff_alpha(
                        [ann['labels'] for ann in sample_annotations]
                    )
                    metrics['krippendorff_alpha'].append(alpha)
                    
                    # ç®€å•ä¸€è‡´æ€§ç™¾åˆ†æ¯”
                    agreement = self.calculate_simple_agreement(sample_annotations)
                    metrics['percentage_agreement'].append(agreement)
            
            # è®¡ç®—å¹³å‡å€¼
            summary = {}
            for metric, values in metrics.items():
                if values:
                    summary[metric] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values)
                    }
            
            return summary
        
        def identify_difficult_samples(self, annotations: List[Dict]):
            """è¯†åˆ«å›°éš¾æ ·æœ¬"""
            
            difficulty_scores = {}
            
            # æŒ‰æ ·æœ¬åˆ†ç»„
            grouped = defaultdict(list)
            for ann in annotations:
                grouped[ann['sample_id']].append(ann)
            
            for sample_id, sample_annotations in grouped.items():
                # è®¡ç®—åˆ†æ­§åº¦
                if len(sample_annotations) > 1:
                    disagreement = 1 - self.calculate_simple_agreement(sample_annotations)
                    
                    # è®¡ç®—æ ‡æ³¨æ—¶é—´
                    avg_time = np.mean([
                        ann['time_spent'] 
                        for ann in sample_annotations
                    ])
                    
                    # ç»¼åˆéš¾åº¦åˆ†æ•°
                    difficulty_scores[sample_id] = {
                        'disagreement': disagreement,
                        'avg_time': avg_time,
                        'num_revisions': sum(ann.get('revisions', 0) 
                                           for ann in sample_annotations),
                        'difficulty_score': disagreement * 0.5 + 
                                          min(avg_time / 60, 1) * 0.3 + 
                                          min(sum(ann.get('revisions', 0) 
                                              for ann in sample_annotations) / 10, 1) * 0.2
                    }
            
            # è¿”å›æœ€å›°éš¾çš„æ ·æœ¬
            return sorted(
                difficulty_scores.items(),
                key=lambda x: x[1]['difficulty_score'],
                reverse=True
            )
    ```
  </Tab>
  
  <Tab value="æ ‡æ³¨å·¥å…·">
    ```python
    class AnnotationInterface:
        """æ ‡æ³¨ç•Œé¢"""
        
        def create_web_interface(self, task_config: Dict):
            """åˆ›å»ºWebæ ‡æ³¨ç•Œé¢"""
            
            from flask import Flask, render_template, request, jsonify
            
            app = Flask(__name__)
            
            @app.route('/')
            def index():
                return render_template('annotation.html', config=task_config)
            
            @app.route('/api/next_sample')
            def next_sample():
                sample = self.get_next_sample(request.args.get('annotator_id'))
                return jsonify(sample)
            
            @app.route('/api/submit_annotation', methods=['POST'])
            def submit_annotation():
                annotation = request.json
                
                # éªŒè¯æ ‡æ³¨
                if not self.validate_annotation_format(annotation):
                    return jsonify({'error': 'Invalid format'}), 400
                
                # ä¿å­˜æ ‡æ³¨
                self.save_annotation(annotation)
                
                # è·å–ä¸‹ä¸€ä¸ªæ ·æœ¬
                next_sample = self.get_next_sample(annotation['annotator_id'])
                
                return jsonify({
                    'status': 'success',
                    'next_sample': next_sample
                })
            
            @app.route('/api/skip_sample', methods=['POST'])
            def skip_sample():
                reason = request.json.get('reason')
                sample_id = request.json.get('sample_id')
                
                self.mark_as_skipped(sample_id, reason)
                
                next_sample = self.get_next_sample(request.json['annotator_id'])
                return jsonify({'next_sample': next_sample})
            
            @app.route('/api/progress')
            def get_progress():
                annotator_id = request.args.get('annotator_id')
                progress = self.calculate_progress(annotator_id)
                return jsonify(progress)
            
            return app
        
        def create_annotation_template(self, task_type: str):
            """åˆ›å»ºæ ‡æ³¨æ¨¡æ¿"""
            
            templates = {
                'classification': '''
                <div class="annotation-container">
                    <div class="sample-display">
                        <h3>è¾“å…¥</h3>
                        <div id="input-text">{{ sample.input }}</div>
                        
                        <h3>è¾“å‡º</h3>
                        <div id="output-text">{{ sample.output }}</div>
                    </div>
                    
                    <div class="annotation-form">
                        <h3>è´¨é‡è¯„åˆ†</h3>
                        <div class="rating-group">
                            <label>å‡†ç¡®æ€§:</label>
                            <input type="range" name="accuracy" min="0" max="5" step="0.5">
                            <span class="rating-value">2.5</span>
                        </div>
                        
                        <div class="rating-group">
                            <label>ç›¸å…³æ€§:</label>
                            <input type="range" name="relevance" min="0" max="5" step="0.5">
                            <span class="rating-value">2.5</span>
                        </div>
                        
                        <div class="rating-group">
                            <label>å®Œæ•´æ€§:</label>
                            <input type="range" name="completeness" min="0" max="5" step="0.5">
                            <span class="rating-value">2.5</span>
                        </div>
                        
                        <h3>åˆ†ç±»æ ‡ç­¾</h3>
                        <div class="checkbox-group">
                            {% for label in labels %}
                            <label>
                                <input type="checkbox" name="labels" value="{{ label }}">
                                {{ label }}
                            </label>
                            {% endfor %}
                        </div>
                        
                        <h3>å¤‡æ³¨</h3>
                        <textarea name="notes" rows="3"></textarea>
                        
                        <div class="button-group">
                            <button onclick="submitAnnotation()">æäº¤</button>
                            <button onclick="skipSample()">è·³è¿‡</button>
                        </div>
                    </div>
                </div>
                ''',
                
                'comparison': '''
                <div class="comparison-container">
                    <h3>é€‰æ‹©æ›´å¥½çš„å“åº”</h3>
                    
                    <div class="input-section">
                        <h4>è¾“å…¥</h4>
                        <div>{{ sample.input }}</div>
                    </div>
                    
                    <div class="responses-section">
                        <div class="response-a">
                            <h4>å“åº” A</h4>
                            <div>{{ sample.response_a }}</div>
                            <button onclick="selectResponse('A')">é€‰æ‹© A</button>
                        </div>
                        
                        <div class="response-b">
                            <h4>å“åº” B</h4>
                            <div>{{ sample.response_b }}</div>
                            <button onclick="selectResponse('B')">é€‰æ‹© B</button>
                        </div>
                    </div>
                    
                    <div class="tie-option">
                        <button onclick="selectResponse('tie')">åŒæ ·å¥½</button>
                    </div>
                    
                    <div class="explanation">
                        <h4>è¯·è§£é‡Šä½ çš„é€‰æ‹©</h4>
                        <textarea name="explanation" rows="3"></textarea>
                    </div>
                </div>
                '''
            }
            
            return templates.get(task_type, templates['classification'])
    ```
  </Tab>
  
  <Tab value="æˆæœ¬ä¼˜åŒ–">
    ```python
    class AnnotationCostOptimizer:
        """æ ‡æ³¨æˆæœ¬ä¼˜åŒ–"""
        
        def optimize_annotation_strategy(self, 
                                        budget: float,
                                        dataset_size: int,
                                        quality_requirement: float):
            """ä¼˜åŒ–æ ‡æ³¨ç­–ç•¥"""
            
            strategies = []
            
            # ç­–ç•¥1ï¼šä¸»åŠ¨å­¦ä¹ 
            active_learning = self.calculate_active_learning_strategy(
                budget, dataset_size, quality_requirement
            )
            strategies.append(active_learning)
            
            # ç­–ç•¥2ï¼šæ··åˆæ ‡æ³¨ï¼ˆäººå·¥+è‡ªåŠ¨ï¼‰
            hybrid = self.calculate_hybrid_strategy(
                budget, dataset_size, quality_requirement
            )
            strategies.append(hybrid)
            
            # ç­–ç•¥3ï¼šæ¸è¿›å¼æ ‡æ³¨
            progressive = self.calculate_progressive_strategy(
                budget, dataset_size, quality_requirement
            )
            strategies.append(progressive)
            
            # é€‰æ‹©æœ€ä¼˜ç­–ç•¥
            best_strategy = max(strategies, key=lambda s: s['expected_quality'] / s['cost'])
            
            return best_strategy
        
        def calculate_active_learning_strategy(self, budget, dataset_size, quality_req):
            """ä¸»åŠ¨å­¦ä¹ ç­–ç•¥"""
            
            # åˆå§‹æ ‡æ³¨æ‰¹æ¬¡
            initial_batch_size = int(dataset_size * 0.1)
            initial_cost = initial_batch_size * self.config['cost_per_annotation']
            
            remaining_budget = budget - initial_cost
            remaining_samples = dataset_size - initial_batch_size
            
            # è¿­ä»£é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬
            selected_samples = []
            current_model_quality = 0.5  # åˆå§‹è´¨é‡
            
            while remaining_budget > 0 and remaining_samples > 0:
                # é€‰æ‹©ä¸ç¡®å®šæ€§æœ€é«˜çš„æ ·æœ¬
                batch_size = min(
                    int(remaining_budget / self.config['cost_per_annotation']),
                    int(remaining_samples * 0.1)
                )
                
                if batch_size == 0:
                    break
                
                # æ›´æ–°æˆæœ¬å’Œè´¨é‡
                batch_cost = batch_size * self.config['cost_per_annotation']
                remaining_budget -= batch_cost
                remaining_samples -= batch_size
                
                # ä¼°è®¡è´¨é‡æå‡
                quality_improvement = self.estimate_quality_improvement(
                    batch_size, 
                    current_model_quality
                )
                current_model_quality += quality_improvement
                
                selected_samples.append({
                    'size': batch_size,
                    'cost': batch_cost,
                    'expected_quality': current_model_quality
                })
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è´¨é‡è¦æ±‚
                if current_model_quality >= quality_req:
                    break
            
            return {
                'strategy': 'active_learning',
                'total_samples': initial_batch_size + sum(b['size'] for b in selected_samples),
                'cost': initial_cost + sum(b['cost'] for b in selected_samples),
                'expected_quality': current_model_quality,
                'iterations': len(selected_samples) + 1
            }
    ```
  </Tab>
</Tabs>

### 3. ç‰ˆæœ¬æ§åˆ¶

```python
class DatasetVersionControl:
    """æ•°æ®é›†ç‰ˆæœ¬æ§åˆ¶"""
    
    def __init__(self, storage_backend='git-lfs'):
        self.storage = self.init_storage(storage_backend)
        self.metadata_db = MetadataDatabase()
    
    def create_version(self, dataset: Dataset, message: str) -> str:
        """åˆ›å»ºæ–°ç‰ˆæœ¬"""
        
        # ç”Ÿæˆç‰ˆæœ¬å·
        version = self.generate_version_number()
        
        # è®¡ç®—æ•°æ®é›†å“ˆå¸Œ
        dataset_hash = self.calculate_dataset_hash(dataset)
        
        # ä¿å­˜æ•°æ®é›†
        storage_path = self.storage.save(
            dataset.to_parquet(),
            f"datasets/{dataset.name}/v{version}"
        )
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'version': version,
            'created_at': datetime.now(),
            'created_by': self.get_current_user(),
            'message': message,
            'dataset_hash': dataset_hash,
            'storage_path': storage_path,
            'statistics': self.calculate_statistics(dataset),
            'parent_version': dataset.parent_version,
            'changes': self.calculate_changes(dataset)
        }
        
        self.metadata_db.save_version(metadata)
        
        # åˆ›å»ºGitæ ‡ç­¾
        if self.storage.type == 'git-lfs':
            self.create_git_tag(version, message)
        
        return version
    
    def compare_versions(self, version1: str, version2: str) -> Dict:
        """æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬"""
        
        # åŠ è½½æ•°æ®é›†
        dataset1 = self.load_version(version1)
        dataset2 = self.load_version(version2)
        
        comparison = {
            'size_change': len(dataset2) - len(dataset1),
            'added_samples': [],
            'removed_samples': [],
            'modified_samples': [],
            'statistics_change': {}
        }
        
        # æ‰¾å‡ºå·®å¼‚
        ids1 = set(dataset1['id'])
        ids2 = set(dataset2['id'])
        
        # æ–°å¢çš„æ ·æœ¬
        added_ids = ids2 - ids1
        comparison['added_samples'] = dataset2[dataset2['id'].isin(added_ids)]
        
        # åˆ é™¤çš„æ ·æœ¬
        removed_ids = ids1 - ids2
        comparison['removed_samples'] = dataset1[dataset1['id'].isin(removed_ids)]
        
        # ä¿®æ”¹çš„æ ·æœ¬
        common_ids = ids1 & ids2
        for sample_id in common_ids:
            sample1 = dataset1[dataset1['id'] == sample_id].iloc[0]
            sample2 = dataset2[dataset2['id'] == sample_id].iloc[0]
            
            if not sample1.equals(sample2):
                comparison['modified_samples'].append({
                    'id': sample_id,
                    'changes': self.diff_samples(sample1, sample2)
                })
        
        # ç»Ÿè®¡å˜åŒ–
        stats1 = self.calculate_statistics(dataset1)
        stats2 = self.calculate_statistics(dataset2)
        
        for key in stats1:
            if key in stats2:
                comparison['statistics_change'][key] = {
                    'before': stats1[key],
                    'after': stats2[key],
                    'change': stats2[key] - stats1[key]
                }
        
        return comparison
    
    def merge_datasets(self, dataset1: Dataset, dataset2: Dataset, 
                      strategy: str = 'union') -> Dataset:
        """åˆå¹¶æ•°æ®é›†"""
        
        if strategy == 'union':
            # å¹¶é›†
            merged = pd.concat([dataset1, dataset2]).drop_duplicates('id')
        
        elif strategy == 'intersection':
            # äº¤é›†
            common_ids = set(dataset1['id']) & set(dataset2['id'])
            merged = dataset1[dataset1['id'].isin(common_ids)]
        
        elif strategy == 'difference':
            # å·®é›†
            unique_ids = set(dataset1['id']) - set(dataset2['id'])
            merged = dataset1[dataset1['id'].isin(unique_ids)]
        
        elif strategy == 'smart':
            # æ™ºèƒ½åˆå¹¶ï¼ˆè§£å†³å†²çªï¼‰
            merged = self.smart_merge(dataset1, dataset2)
        
        else:
            raise ValueError(f"Unknown merge strategy: {strategy}")
        
        # æ›´æ–°å…ƒæ•°æ®
        merged.metadata = {
            'merge_info': {
                'dataset1': dataset1.metadata,
                'dataset2': dataset2.metadata,
                'strategy': strategy,
                'merge_date': datetime.now()
            }
        }
        
        return merged
```

## æ•°æ®è´¨é‡ç®¡ç†

### è´¨é‡æ£€æŸ¥ä½“ç³»

```python
class DataQualityManager:
    """æ•°æ®è´¨é‡ç®¡ç†"""
    
    def __init__(self):
        self.quality_checks = self.init_quality_checks()
        self.quality_metrics = {}
    
    def comprehensive_quality_check(self, dataset: Dataset) -> Dict:
        """å…¨é¢è´¨é‡æ£€æŸ¥"""
        
        print("ğŸ” Running comprehensive quality check...")
        
        results = {
            'overall_score': 0,
            'checks': {},
            'issues': [],
            'recommendations': []
        }
        
        # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
        for check_name, check_func in self.quality_checks.items():
            check_result = check_func(dataset)
            results['checks'][check_name] = check_result
            
            # è®°å½•é—®é¢˜
            if check_result['issues']:
                results['issues'].extend(check_result['issues'])
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        results['overall_score'] = self.calculate_quality_score(results['checks'])
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        results['recommendations'] = self.generate_recommendations(results)
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        self.generate_quality_report(results)
        
        return results
    
    def check_data_completeness(self, dataset: Dataset) -> Dict:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        
        completeness_issues = []
        
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = ['id', 'input', 'output', 'timestamp']
        for field in required_fields:
            if field not in dataset.columns:
                completeness_issues.append(f"Missing required field: {field}")
            else:
                # æ£€æŸ¥ç©ºå€¼
                null_count = dataset[field].isnull().sum()
                if null_count > 0:
                    completeness_issues.append(
                        f"Field '{field}' has {null_count} null values"
                    )
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        expected_types = {
            'id': 'string',
            'input': 'string',
            'output': 'string',
            'timestamp': 'datetime'
        }
        
        for field, expected_type in expected_types.items():
            if field in dataset.columns:
                actual_type = str(dataset[field].dtype)
                if not self.check_type_compatibility(actual_type, expected_type):
                    completeness_issues.append(
                        f"Field '{field}' has wrong type: {actual_type} (expected {expected_type})"
                    )
        
        return {
            'score': 1.0 - len(completeness_issues) / 10,  # å‡è®¾10ä¸ªé—®é¢˜ä¸ºæœ€å·®
            'issues': completeness_issues
        }
    
    def check_data_consistency(self, dataset: Dataset) -> Dict:
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        
        consistency_issues = []
        
        # æ£€æŸ¥IDå”¯ä¸€æ€§
        duplicate_ids = dataset[dataset.duplicated('id')]['id'].tolist()
        if duplicate_ids:
            consistency_issues.append(f"Duplicate IDs found: {duplicate_ids[:5]}...")
        
        # æ£€æŸ¥æ—¶é—´æˆ³é¡ºåº
        if 'timestamp' in dataset.columns:
            unsorted_count = (dataset['timestamp'].diff() < 0).sum()
            if unsorted_count > 0:
                consistency_issues.append(
                    f"{unsorted_count} samples have incorrect timestamp order"
                )
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦å¼‚å¸¸
        if 'input' in dataset.columns:
            input_lengths = dataset['input'].str.len()
            outliers = self.detect_outliers(input_lengths)
            if len(outliers) > 0:
                consistency_issues.append(
                    f"{len(outliers)} samples have abnormal input length"
                )
        
        return {
            'score': 1.0 - len(consistency_issues) / 5,
            'issues': consistency_issues
        }
    
    def check_label_quality(self, dataset: Dataset) -> Dict:
        """æ£€æŸ¥æ ‡ç­¾è´¨é‡"""
        
        if 'labels' not in dataset.columns:
            return {'score': 0, 'issues': ['No labels found']}
        
        label_issues = []
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        label_distribution = dataset['labels'].value_counts()
        
        # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡
        imbalance_ratio = label_distribution.max() / label_distribution.min()
        if imbalance_ratio > 10:
            label_issues.append(
                f"Severe class imbalance detected (ratio: {imbalance_ratio:.1f})"
            )
        
        # æ£€æŸ¥ç½•è§æ ‡ç­¾
        rare_labels = label_distribution[label_distribution < 5].index.tolist()
        if rare_labels:
            label_issues.append(
                f"Rare labels found: {rare_labels}"
            )
        
        # æ£€æŸ¥æ ‡ç­¾ä¸€è‡´æ€§ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ ‡æ³¨è€…ï¼‰
        if 'annotator_id' in dataset.columns:
            agreement = self.calculate_label_agreement(dataset)
            if agreement < 0.7:
                label_issues.append(
                    f"Low inter-annotator agreement: {agreement:.2f}"
                )
        
        return {
            'score': 1.0 - len(label_issues) / 3,
            'issues': label_issues,
            'distribution': label_distribution.to_dict()
        }
```

### æ•°æ®æ¸…æ´—æµæ°´çº¿

```python
class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—æµæ°´çº¿"""
    
    def __init__(self):
        self.cleaners = []
        self.setup_pipeline()
    
    def setup_pipeline(self):
        """è®¾ç½®æ¸…æ´—æµæ°´çº¿"""
        
        self.add_cleaner(RemoveDuplicatesCleaner())
        self.add_cleaner(RemovePIICleaner())
        self.add_cleaner(NormalizeTextCleaner())
        self.add_cleaner(FilterQualityCleaner())
        self.add_cleaner(BalanceDatasetCleaner())
    
    def clean(self, dataset: Dataset) -> Dataset:
        """æ‰§è¡Œæ¸…æ´—"""
        
        cleaned = dataset.copy()
        cleaning_log = []
        
        for cleaner in self.cleaners:
            print(f"Running {cleaner.__class__.__name__}...")
            
            before_size = len(cleaned)
            cleaned = cleaner.clean(cleaned)
            after_size = len(cleaned)
            
            cleaning_log.append({
                'cleaner': cleaner.__class__.__name__,
                'removed': before_size - after_size,
                'remaining': after_size
            })
        
        # æ·»åŠ æ¸…æ´—å…ƒæ•°æ®
        cleaned.metadata['cleaning_log'] = cleaning_log
        cleaned.metadata['cleaned_at'] = datetime.now()
        
        return cleaned

class RemovePIICleaner:
    """PIIæ¸…æ´—å™¨"""
    
    def __init__(self):
        self.pii_patterns = self.load_pii_patterns()
    
    def clean(self, dataset: Dataset) -> Dataset:
        """ç§»é™¤PIIä¿¡æ¯"""
        
        cleaned = dataset.copy()
        
        for column in ['input', 'output']:
            if column in cleaned.columns:
                cleaned[column] = cleaned[column].apply(self.remove_pii)
        
        return cleaned
    
    def remove_pii(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­ç§»é™¤PII"""
        
        # ç”µå­é‚®ä»¶
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', 
                     '[EMAIL]', text)
        
        # ç”µè¯å·ç 
        text = re.sub(r'(\+?\d{1,3}[-.\s]?)?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}',
                     '[PHONE]', text)
        
        # èº«ä»½è¯å·
        text = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', '[SSN]', text)
        
        # ä¿¡ç”¨å¡
        text = re.sub(r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b', '[CREDIT_CARD]', text)
        
        # IPåœ°å€
        text = re.sub(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', '[IP_ADDRESS]', text)
        
        # äººåï¼ˆä½¿ç”¨NERæ¨¡å‹ï¼‰
        # text = self.remove_names(text)
        
        return text
```

## æ•°æ®é›†åˆ†æä¸æ´å¯Ÿ

```python
class DatasetAnalyzer:
    """æ•°æ®é›†åˆ†æå™¨"""
    
    def analyze_dataset(self, dataset: Dataset) -> Dict:
        """åˆ†ææ•°æ®é›†"""
        
        analysis = {
            'basic_stats': self.calculate_basic_stats(dataset),
            'distribution': self.analyze_distribution(dataset),
            'diversity': self.analyze_diversity(dataset),
            'coverage': self.analyze_coverage(dataset),
            'quality_metrics': self.analyze_quality(dataset),
            'insights': self.generate_insights(dataset)
        }
        
        return analysis
    
    def analyze_diversity(self, dataset: Dataset) -> Dict:
        """åˆ†æå¤šæ ·æ€§"""
        
        diversity_metrics = {}
        
        # è¯æ±‡å¤šæ ·æ€§
        all_text = ' '.join(dataset['input'].tolist() + dataset['output'].tolist())
        tokens = all_text.split()
        
        diversity_metrics['vocabulary_size'] = len(set(tokens))
        diversity_metrics['type_token_ratio'] = len(set(tokens)) / len(tokens)
        
        # è¯­ä¹‰å¤šæ ·æ€§
        embeddings = self.compute_embeddings(dataset['input'].tolist())
        diversity_metrics['semantic_diversity'] = self.calculate_semantic_diversity(embeddings)
        
        # é•¿åº¦å¤šæ ·æ€§
        lengths = dataset['input'].str.len()
        diversity_metrics['length_diversity'] = {
            'mean': lengths.mean(),
            'std': lengths.std(),
            'min': lengths.min(),
            'max': lengths.max(),
            'cv': lengths.std() / lengths.mean()  # å˜å¼‚ç³»æ•°
        }
        
        # ä¸»é¢˜å¤šæ ·æ€§
        topics = self.extract_topics(dataset)
        diversity_metrics['topic_diversity'] = {
            'num_topics': len(topics),
            'topic_distribution': topics
        }
        
        return diversity_metrics
    
    def analyze_coverage(self, dataset: Dataset) -> Dict:
        """åˆ†æè¦†ç›–åº¦"""
        
        coverage = {}
        
        # ä»»åŠ¡ç±»å‹è¦†ç›–
        if 'task_type' in dataset.columns:
            coverage['task_types'] = dataset['task_type'].value_counts().to_dict()
        
        # éš¾åº¦çº§åˆ«è¦†ç›–
        if 'difficulty' in dataset.columns:
            coverage['difficulty_levels'] = dataset['difficulty'].value_counts().to_dict()
        
        # é¢†åŸŸè¦†ç›–
        if 'domain' in dataset.columns:
            coverage['domains'] = dataset['domain'].value_counts().to_dict()
        
        # è¾¹ç¼˜æ¡ˆä¾‹è¦†ç›–
        coverage['edge_cases'] = self.identify_edge_cases(dataset)
        
        # è®¡ç®—è¦†ç›–åˆ†æ•°
        coverage['coverage_score'] = self.calculate_coverage_score(coverage)
        
        return coverage
    
    def generate_insights(self, dataset: Dataset) -> List[str]:
        """ç”Ÿæˆæ´å¯Ÿ"""
        
        insights = []
        
        # æ•°æ®è§„æ¨¡æ´å¯Ÿ
        if len(dataset) < 100:
            insights.append("âš ï¸ æ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼Œå¯èƒ½å½±å“è¯„ä¼°çš„ç»Ÿè®¡æ˜¾è‘—æ€§")
        elif len(dataset) > 10000:
            insights.append("âœ… æ•°æ®é›†è§„æ¨¡å……è¶³ï¼Œé€‚åˆè¿›è¡Œå…¨é¢è¯„ä¼°")
        
        # è´¨é‡æ´å¯Ÿ
        quality_score = self.analyze_quality(dataset)['overall_score']
        if quality_score < 0.7:
            insights.append("âš ï¸ æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—")
        
        # å¤šæ ·æ€§æ´å¯Ÿ
        diversity = self.analyze_diversity(dataset)
        if diversity['type_token_ratio'] < 0.1:
            insights.append("âš ï¸ è¯æ±‡å¤šæ ·æ€§è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨é‡å¤æˆ–æ¨¡æ¿åŒ–å†…å®¹")
        
        # å¹³è¡¡æ€§æ´å¯Ÿ
        if 'labels' in dataset.columns:
            imbalance = self.check_class_imbalance(dataset)
            if imbalance > 5:
                insights.append(f"âš ï¸ ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼ˆæ¯”ä¾‹ {imbalance:.1f}:1ï¼‰ï¼Œå»ºè®®é‡æ–°é‡‡æ ·")
        
        return insights
```

## æ•°æ®é›†å¯è§†åŒ–

```python
class DatasetVisualizer:
    """æ•°æ®é›†å¯è§†åŒ–"""
    
    def create_dashboard(self, dataset: Dataset):
        """åˆ›å»ºæ•°æ®é›†ä»ªè¡¨æ¿"""
        
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=(
                'æ ·æœ¬æ•°é‡è¶‹åŠ¿', 'é•¿åº¦åˆ†å¸ƒ', 'æ ‡ç­¾åˆ†å¸ƒ',
                'è´¨é‡åˆ†æ•°åˆ†å¸ƒ', 'å¤šæ ·æ€§æŒ‡æ ‡', 'è¦†ç›–åº¦çƒ­å›¾',
                'æ—¶é—´åˆ†å¸ƒ', 'éš¾åº¦åˆ†å¸ƒ', 'ç›¸ä¼¼åº¦çŸ©é˜µ'
            ),
            specs=[
                [{'type': 'scatter'}, {'type': 'histogram'}, {'type': 'bar'}],
                [{'type': 'histogram'}, {'type': 'scatter'}, {'type': 'heatmap'}],
                [{'type': 'scatter'}, {'type': 'pie'}, {'type': 'heatmap'}]
            ]
        )
        
        # æ·»åŠ å„ç§å›¾è¡¨...
        
        return fig
```

## æœ€ä½³å®è·µ

<Cards>
  <Card title="æŒç»­æ›´æ–°">
    å®šæœŸæ›´æ–°æ•°æ®é›†ï¼Œä¿æŒä¸å®é™…ä½¿ç”¨åœºæ™¯åŒæ­¥
  </Card>
  
  <Card title="ç‰ˆæœ¬ç®¡ç†">
    ä¸¥æ ¼çš„ç‰ˆæœ¬æ§åˆ¶ï¼Œç¡®ä¿å¯è¿½æº¯å’Œå¯é‡ç°
  </Card>
  
  <Card title="è´¨é‡ç¬¬ä¸€">
    è´¨é‡ä¼˜äºæ•°é‡ï¼Œç¡®ä¿æ•°æ®é›†çš„é«˜è´¨é‡
  </Card>
  
  <Card title="å¤šæ ·æ€§ä¿è¯">
    ç¡®ä¿æ•°æ®é›†è¦†ç›–å„ç§åœºæ™¯å’Œè¾¹ç¼˜æ¡ˆä¾‹
  </Card>
</Cards>

## å…³é”®è¦ç‚¹

- âœ… å»ºç«‹å®Œæ•´çš„æ•°æ®é›†ç”Ÿå‘½å‘¨æœŸç®¡ç†æµç¨‹
- âœ… å®æ–½ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶å’Œæ ‡æ³¨è§„èŒƒ
- âœ… ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç¡®ä¿å¯è¿½æº¯æ€§
- âœ… å®šæœŸåˆ†æå’Œä¼˜åŒ–æ•°æ®é›†
- âœ… å¹³è¡¡æˆæœ¬å’Œè´¨é‡ï¼Œä¼˜åŒ–æ ‡æ³¨ç­–ç•¥