---
title: LLM 作为评判者
description: 深入理解如何使用大语言模型进行自动化评估，包括原理、实践和优化技巧
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

# LLM 作为评判者（LLM-as-Judge）

使用大语言模型评估其他模型输出是当前最流行和实用的评估方法之一。本章将深入探讨这种方法的原理、实现和最佳实践。

## 核心原理

### 工作机制

<Mermaid
  chart="
graph LR
    A[用户输入] --> B[目标模型]
    B --> C[模型输出]
    C --> D[评判 Prompt 构建]
    A --> D
    D --> E[评判模型]
    E --> F[评估结果]
    F --> G[分数 + 反馈]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#9f9,stroke:#333,stroke-width:2px
"
/>

### 理论基础

<Callout type="info">
**核心假设**：强大的语言模型（如 GPT-4）具有足够的理解和判断能力，可以根据给定标准评估文本质量。
</Callout>

## 评判模型选择

### 模型对比

| 模型 | 准确性 | 成本 | 速度 | 推荐场景 |
|------|--------|------|------|----------|
| **GPT-4** | ⭐⭐⭐⭐⭐ | 💰💰💰💰 | 🐢 | 高价值评估、基准建立 |
| **GPT-3.5-Turbo** | ⭐⭐⭐⭐ | 💰💰 | 🐰 | 日常评估、批量测试 |
| **Claude 3** | ⭐⭐⭐⭐⭐ | 💰💰💰 | 🐢 | 复杂推理、安全评估 |
| **Gemini Pro** | ⭐⭐⭐⭐ | 💰💰 | 🐰 | 多模态评估 |
| **Llama 3 70B** | ⭐⭐⭐ | 💰 | 🐰 | 本地部署、隐私敏感 |
| **Qwen 2** | ⭐⭐⭐ | 💰 | 🚀 | 中文评估、成本优化 |

### 选择策略

```python
def select_judge_model(evaluation_context):
    """
    根据评估场景选择合适的评判模型
    """
    if evaluation_context["budget"] == "high" and evaluation_context["accuracy_required"] == "critical":
        return "gpt-4"
    
    elif evaluation_context["language"] == "chinese":
        if evaluation_context["local_deployment"]:
            return "qwen-2-72b"
        else:
            return "gpt-3.5-turbo"  # 或 claude-3
    
    elif evaluation_context["task_type"] == "safety":
        return "claude-3-opus"  # Claude 在安全性评估上表现优秀
    
    elif evaluation_context["volume"] > 10000:
        return "gpt-3.5-turbo"  # 平衡成本和质量
    
    else:
        return "gpt-3.5-turbo"  # 默认选择
```

## Prompt 工程最佳实践

### 1. 基础 Prompt 模板

```python
BASIC_EVALUATION_PROMPT = """
你是一位专业的内容评审专家。请根据以下标准评估给定的输出质量。

## 评估任务
输入问题：{input}
模型输出：{output}

## 评估标准
1. 准确性（30分）：回答是否正确、无误
2. 相关性（25分）：是否直接回答了问题
3. 完整性（20分）：是否涵盖所有重要方面
4. 清晰度（15分）：表达是否清楚易懂
5. 实用性（10分）：答案是否有实际价值

## 输出要求
请提供：
1. 各维度得分（满分如上所示）
2. 总分（100分制）
3. 优点（至少2点）
4. 不足（如有）
5. 改进建议（如有）

请以JSON格式输出评估结果。
"""
```

### 2. 高级 Prompt 技巧

<Tabs items={['Chain-of-Thought', 'Few-Shot', 'Rubric-Based', 'Comparative']}>
  <Tab value="Chain-of-Thought">
    ```python
    COT_EVALUATION_PROMPT = """
    请作为评审专家，通过逐步分析来评估以下输出。
    
    输入：{input}
    输出：{output}
    
    评估步骤：
    
    第1步：理解用户意图
    - 用户真正想要什么？
    - 关键需求是什么？
    
    第2步：分析输出内容
    - 输出回答了哪些方面？
    - 是否有遗漏或错误？
    
    第3步：评估质量维度
    - 准确性如何？（提供具体例子）
    - 完整性如何？（列出缺失内容）
    - 表达清晰吗？（指出不清楚的地方）
    
    第4步：综合评分
    基于以上分析，给出最终评分和理由。
    
    请展示你的思考过程，然后给出最终评分（0-100）。
    """
    ```
  </Tab>
  
  <Tab value="Few-Shot">
    ```python
    FEW_SHOT_PROMPT = """
    请参考以下示例来评估新的输出：
    
    ## 示例1
    输入：什么是机器学习？
    输出：机器学习是AI的一个分支，让计算机从数据中学习。
    评分：75/100
    理由：回答正确但过于简单，缺少具体例子和应用场景。
    
    ## 示例2
    输入：如何学习Python？
    输出：学习Python可以：1)从基础语法开始 2)练习项目 3)阅读文档 4)参与社区
    评分：90/100
    理由：回答全面、结构清晰、实用性强。
    
    ## 待评估
    输入：{input}
    输出：{output}
    
    请按照示例格式给出评分和理由。
    """
    ```
  </Tab>
  
  <Tab value="Rubric-Based">
    ```python
    RUBRIC_PROMPT = """
    使用以下评分标准表评估输出：
    
    ## 评分标准表（Rubric）
    
    ### 准确性 (0-40分)
    - 优秀(36-40): 完全准确，无任何错误
    - 良好(28-35): 基本准确，有极少错误
    - 中等(20-27): 部分准确，有一些错误
    - 差(0-19): 错误较多或完全错误
    
    ### 相关性 (0-30分)
    - 优秀(27-30): 完全切题，直接回答问题
    - 良好(21-26): 基本切题，略有偏离
    - 中等(15-20): 部分相关，偏离较多
    - 差(0-14): 基本不相关或答非所问
    
    ### 深度 (0-30分)
    - 优秀(27-30): 深入透彻，有独特见解
    - 良好(21-26): 有一定深度，分析合理
    - 中等(15-20): 表面化，缺乏深度
    - 差(0-14): 极其肤浅或无分析
    
    输入：{input}
    输出：{output}
    
    请根据标准表给出各维度得分和总分。
    """
    ```
  </Tab>
  
  <Tab value="Comparative">
    ```python
    COMPARATIVE_PROMPT = """
    请比较以下两个输出，选出更好的一个：
    
    输入问题：{input}
    
    输出A：{output_a}
    输出B：{output_b}
    
    比较维度：
    1. 准确性
    2. 完整性
    3. 清晰度
    4. 实用性
    
    请分析：
    - 输出A的优势
    - 输出B的优势
    - 哪个更好及原因
    - 改进建议
    
    最终选择：[A/B]
    置信度：[0-1]
    """
    ```
  </Tab>
</Tabs>

### 3. Prompt 优化技巧

<Accordions>
  <Accordion title="明确评估标准">
    ```python
    # ❌ 模糊的标准
    "请评估这个回答的质量"
    
    # ✅ 明确的标准
    """
    请根据以下具体标准评估：
    1. 事实准确性：所有陈述都有据可查
    2. 逻辑连贯性：论述有条理，前后一致
    3. 示例质量：提供相关且有说服力的例子
    """
    ```
  </Accordion>
  
  <Accordion title="提供评分锚点">
    ```python
    # 定义清晰的评分锚点
    """
    评分标准：
    - 10分：完美回答，超出预期
    - 8-9分：优秀，完全满足要求
    - 6-7分：良好，基本满足要求
    - 4-5分：及格，部分满足要求
    - 2-3分：较差，大部分不满足
    - 0-1分：完全不满足要求
    """
    ```
  </Accordion>
  
  <Accordion title="要求结构化输出">
    ```python
    # 使用JSON格式确保输出可解析
    """
    请以以下JSON格式输出：
    {
        "total_score": <0-100的数字>,
        "dimensions": {
            "accuracy": <分数>,
            "relevance": <分数>,
            "completeness": <分数>
        },
        "strengths": ["优点1", "优点2"],
        "weaknesses": ["不足1", "不足2"],
        "suggestions": "改进建议"
    }
    """
    ```
  </Accordion>
  
  <Accordion title="处理边界情况">
    ```python
    # 添加特殊情况处理
    """
    特殊情况处理：
    - 如果输出包含有害内容，直接给0分
    - 如果输出完全偏题，最高不超过20分
    - 如果无法理解输出，请标记为"需要人工审核"
    """
    ```
  </Accordion>
</Accordions>

## 实现框架

### 完整的 LLM-as-Judge 系统

```python
import json
import asyncio
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum

class EvaluationDimension(Enum):
    ACCURACY = "accuracy"
    RELEVANCE = "relevance"
    COMPLETENESS = "completeness"
    CLARITY = "clarity"
    SAFETY = "safety"

@dataclass
class EvaluationConfig:
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.0
    max_retries: int = 3
    timeout: int = 30
    dimensions: List[EvaluationDimension] = None
    
    def __post_init__(self):
        if self.dimensions is None:
            self.dimensions = list(EvaluationDimension)

class LLMJudge:
    """
    LLM-as-Judge 评估系统
    """
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.prompt_template = self._load_prompt_template()
        self.client = self._initialize_client()
    
    def _load_prompt_template(self) -> str:
        """加载评估提示词模板"""
        return """
        作为专业评审员，请评估以下内容：
        
        输入：{input}
        输出：{output}
        
        评估维度：{dimensions}
        
        请提供：
        1. 各维度评分（0-10）
        2. 总体评分（0-100）
        3. 详细反馈
        
        输出JSON格式。
        """
    
    async def evaluate(self, input_text: str, output_text: str, 
                       reference: Optional[str] = None) -> Dict:
        """
        执行评估
        """
        # 构建评估prompt
        prompt = self._build_prompt(input_text, output_text, reference)
        
        # 调用评判模型
        for attempt in range(self.config.max_retries):
            try:
                response = await self._call_judge_model(prompt)
                result = self._parse_response(response)
                
                # 验证结果
                if self._validate_result(result):
                    return self._enrich_result(result, input_text, output_text)
                
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)  # 指数退避
        
        raise ValueError("Failed to get valid evaluation after retries")
    
    def _build_prompt(self, input_text: str, output_text: str, 
                     reference: Optional[str]) -> str:
        """构建评估提示词"""
        dimensions_str = ", ".join([d.value for d in self.config.dimensions])
        
        prompt = self.prompt_template.format(
            input=input_text,
            output=output_text,
            dimensions=dimensions_str
        )
        
        if reference:
            prompt += f"\n参考答案：{reference}"
        
        return prompt
    
    async def _call_judge_model(self, prompt: str) -> str:
        """调用评判模型"""
        response = await self.client.chat.completions.create(
            model=self.config.model,
            messages=[
                {"role": "system", "content": "You are a professional evaluator."},
                {"role": "user", "content": prompt}
            ],
            temperature=self.config.temperature,
            response_format={"type": "json_object"}  # 强制JSON输出
        )
        
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> Dict:
        """解析模型响应"""
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            # 尝试修复常见的JSON错误
            fixed = self._fix_json(response)
            return json.loads(fixed)
    
    def _validate_result(self, result: Dict) -> bool:
        """验证评估结果"""
        required_fields = ["total_score", "dimensions", "feedback"]
        
        # 检查必需字段
        for field in required_fields:
            if field not in result:
                return False
        
        # 检查分数范围
        if not 0 <= result["total_score"] <= 100:
            return False
        
        # 检查维度分数
        for dim in self.config.dimensions:
            if dim.value not in result["dimensions"]:
                return False
            if not 0 <= result["dimensions"][dim.value] <= 10:
                return False
        
        return True
    
    def _enrich_result(self, result: Dict, input_text: str, 
                      output_text: str) -> Dict:
        """丰富评估结果"""
        result["metadata"] = {
            "model": self.config.model,
            "timestamp": datetime.now().isoformat(),
            "input_length": len(input_text),
            "output_length": len(output_text),
            "evaluation_version": "1.0"
        }
        
        # 计算额外指标
        result["confidence"] = self._calculate_confidence(result)
        result["quality_tier"] = self._determine_quality_tier(result["total_score"])
        
        return result
    
    def _calculate_confidence(self, result: Dict) -> float:
        """计算评估置信度"""
        # 基于维度分数的一致性计算置信度
        scores = list(result["dimensions"].values())
        if not scores:
            return 0.5
        
        mean = sum(scores) / len(scores)
        variance = sum((s - mean) ** 2 for s in scores) / len(scores)
        
        # 方差越小，置信度越高
        confidence = 1 / (1 + variance / 10)
        return round(confidence, 2)
    
    def _determine_quality_tier(self, score: float) -> str:
        """确定质量等级"""
        if score >= 90:
            return "excellent"
        elif score >= 75:
            return "good"
        elif score >= 60:
            return "acceptable"
        elif score >= 40:
            return "poor"
        else:
            return "unacceptable"

# 使用示例
async def main():
    config = EvaluationConfig(
        model="gpt-4",
        temperature=0.0,
        dimensions=[
            EvaluationDimension.ACCURACY,
            EvaluationDimension.RELEVANCE,
            EvaluationDimension.CLARITY
        ]
    )
    
    judge = LLMJudge(config)
    
    result = await judge.evaluate(
        input_text="什么是机器学习？",
        output_text="机器学习是人工智能的一个分支...",
        reference="机器学习是一种让计算机从数据中学习的方法..."
    )
    
    print(json.dumps(result, indent=2, ensure_ascii=False))

# asyncio.run(main())
```

## 常见挑战与解决方案

### 1. 评分一致性问题

<Callout type="warning">
**问题**：同样的输入，多次评估得分差异较大
</Callout>

**解决方案**：

```python
class ConsistentJudge:
    """
    提高评分一致性的评判器
    """
    
    def __init__(self):
        self.temperature = 0.0  # 使用确定性输出
        self.seed = 42  # 固定随机种子
    
    async def evaluate_with_voting(self, input_text, output_text, n_judges=3):
        """
        多个评判投票机制
        """
        scores = []
        
        for i in range(n_judges):
            # 使用略微不同的prompt
            prompt_variant = self._create_prompt_variant(i)
            score = await self._single_evaluation(
                input_text, 
                output_text, 
                prompt_variant
            )
            scores.append(score)
        
        # 计算中位数作为最终分数
        final_score = np.median(scores)
        
        # 计算一致性指标
        consistency = 1 - (np.std(scores) / np.mean(scores))
        
        return {
            "final_score": final_score,
            "all_scores": scores,
            "consistency": consistency
        }
```

### 2. 成本优化

```python
class CostOptimizedJudge:
    """
    成本优化的评判策略
    """
    
    def __init__(self):
        self.cheap_model = "gpt-3.5-turbo"
        self.expensive_model = "gpt-4"
        self.confidence_threshold = 0.8
    
    async def hierarchical_evaluation(self, input_text, output_text):
        """
        分层评估：先用便宜模型，不确定时才用贵模型
        """
        # 第一层：便宜模型快速评估
        cheap_result = await self.evaluate_with_model(
            self.cheap_model, 
            input_text, 
            output_text
        )
        
        # 如果置信度高，直接返回
        if cheap_result["confidence"] > self.confidence_threshold:
            return cheap_result
        
        # 第二层：贵模型精确评估
        expensive_result = await self.evaluate_with_model(
            self.expensive_model, 
            input_text, 
            output_text
        )
        
        return expensive_result
```

### 3. 处理长文本

```python
class LongTextJudge:
    """
    处理长文本的评判策略
    """
    
    def __init__(self, max_chunk_size=2000):
        self.max_chunk_size = max_chunk_size
    
    async def evaluate_long_text(self, input_text, output_text):
        """
        分段评估长文本
        """
        # 分割文本
        chunks = self._split_text(output_text, self.max_chunk_size)
        
        # 评估每个片段
        chunk_scores = []
        for i, chunk in enumerate(chunks):
            score = await self.evaluate_chunk(
                input_text, 
                chunk, 
                chunk_index=i, 
                total_chunks=len(chunks)
            )
            chunk_scores.append(score)
        
        # 聚合结果
        final_result = self._aggregate_chunk_scores(chunk_scores)
        
        return final_result
    
    def _split_text(self, text, max_size):
        """智能分割文本，保持语义完整"""
        # 实现智能分割逻辑
        pass
```

## 评估评判质量

### 元评估（Meta-Evaluation）

```python
class MetaEvaluator:
    """
    评估评判器本身的质量
    """
    
    def __init__(self):
        self.human_labels = {}  # 人工标注的黄金标准
    
    def evaluate_judge_quality(self, judge, test_set):
        """
        评估评判器质量
        """
        metrics = {
            "accuracy": 0,
            "precision": 0,
            "recall": 0,
            "correlation": 0
        }
        
        predictions = []
        ground_truth = []
        
        for test_case in test_set:
            # 获取评判器预测
            pred = judge.evaluate(
                test_case["input"], 
                test_case["output"]
            )
            predictions.append(pred["score"])
            
            # 获取人工标注
            truth = self.human_labels[test_case["id"]]
            ground_truth.append(truth)
        
        # 计算相关性
        metrics["correlation"] = np.corrcoef(predictions, ground_truth)[0, 1]
        
        # 计算分类指标（将分数转为类别）
        pred_classes = [self._score_to_class(s) for s in predictions]
        truth_classes = [self._score_to_class(s) for s in ground_truth]
        
        metrics["accuracy"] = accuracy_score(truth_classes, pred_classes)
        metrics["precision"] = precision_score(truth_classes, pred_classes, average='macro')
        metrics["recall"] = recall_score(truth_classes, pred_classes, average='macro')
        
        return metrics
```

## 最佳实践建议

<Cards>
  <Card title="使用多个评判维度">
    不要只用单一分数，而是评估多个维度，提供更全面的反馈
  </Card>
  
  <Card title="定期校准">
    定期用人工标注数据校准评判模型，确保评分标准不偏移
  </Card>
  
  <Card title="记录元数据">
    记录评估的所有相关信息，便于后续分析和改进
  </Card>
  
  <Card title="建立反馈循环">
    收集评估结果的反馈，持续优化评判prompt和策略
  </Card>
</Cards>

## 工具和资源

### 开源工具

- **LangChain Evaluators**: 提供多种预置的评估器
- **Ragas**: 专门用于 RAG 系统的评估框架
- **OpenAI Evals**: OpenAI 的评估框架
- **Promptfoo**: 用于测试和评估 LLM 的工具

### 示例代码库

```bash
# 克隆示例代码
git clone https://github.com/your-org/llm-judge-examples

# 安装依赖
pip install -r requirements.txt

# 运行示例
python examples/basic_judge.py
```

## 关键要点总结

- ✅ LLM-as-Judge 是当前最实用的自动评估方法
- ✅ Prompt 设计是成功的关键
- ✅ 需要平衡成本和准确性
- ✅ 多维度评估比单一分数更有价值
- ✅ 定期校准和验证必不可少

## 下一步

<Card title="人工标注系统 →" href="/docs/evaluation-methods/human-annotation" description="了解如何构建高质量的人工标注流程" />