---
title: å¿«é€Ÿå¼€å§‹
description: 10åˆ†é’Ÿå†…å®Œæˆä½ çš„ç¬¬ä¸€ä¸ª LLM è¯„ä¼°ï¼Œå¿«é€Ÿä¸Šæ‰‹è¯„ä¼°ç³»ç»Ÿ
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';
import { CodeBlock } from 'fumadocs-ui/components/codeblock';

# å¿«é€Ÿå¼€å§‹ï¼š10åˆ†é’Ÿå®Œæˆç¬¬ä¸€ä¸ªè¯„ä¼°

æœ¬æ•™ç¨‹å°†å¸¦ä½ åœ¨ 10 åˆ†é’Ÿå†…å®Œæˆç¬¬ä¸€ä¸ª LLM è¯„ä¼°ï¼Œè®©ä½ å¿«é€Ÿç†è§£è¯„ä¼°çš„åŸºæœ¬æµç¨‹ã€‚

<Callout type="info">
**å‡†å¤‡æ—¶é—´**ï¼š2 åˆ†é’Ÿ | **ç¼–ç æ—¶é—´**ï¼š5 åˆ†é’Ÿ | **è¿è¡Œæ—¶é—´**ï¼š3 åˆ†é’Ÿ
</Callout>

## ç¯å¢ƒå‡†å¤‡ï¼ˆ2åˆ†é’Ÿï¼‰

### å®‰è£…å¿…è¦çš„åŒ…

<Tabs items={['pip', 'conda', 'poetry']}>
  <Tab value="pip">
    ```bash
    # å®‰è£…åŸºç¡€ä¾èµ–
    pip install openai langfuse pandas numpy

    # å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹
    pip install transformers torch
    ```
  </Tab>
  
  <Tab value="conda">
    ```bash
    # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
    conda create -n llm-eval python=3.9
    conda activate llm-eval

    # å®‰è£…ä¾èµ–
    conda install -c conda-forge openai pandas numpy
    pip install langfuse
    ```
  </Tab>
  
  <Tab value="poetry">
    ```bash
    # åˆå§‹åŒ–é¡¹ç›®
    poetry init
    
    # æ·»åŠ ä¾èµ–
    poetry add openai langfuse pandas numpy
    ```
  </Tab>
</Tabs>

### é…ç½®ç¯å¢ƒå˜é‡

```bash
# åˆ›å»º .env æ–‡ä»¶
touch .env

# æ·»åŠ å¿…è¦çš„ API å¯†é’¥
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
echo "LANGFUSE_PUBLIC_KEY=your_public_key" >> .env
echo "LANGFUSE_SECRET_KEY=your_secret_key" >> .env
```

## ç¬¬ä¸€ä¸ªè¯„ä¼°ç¤ºä¾‹ï¼ˆ5åˆ†é’Ÿï¼‰

### Step 1: å‡†å¤‡æµ‹è¯•æ•°æ®

åˆ›å»ºæ–‡ä»¶ `first_evaluation.py`ï¼š

```python
# first_evaluation.py
import os
from dotenv import load_dotenv
import openai
from langfuse import Langfuse
import json
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ–å®¢æˆ·ç«¯
openai.api_key = os.getenv("OPENAI_API_KEY")
langfuse = Langfuse()

# å‡†å¤‡æµ‹è¯•æ•°æ®é›†
test_cases = [
    {
        "id": "test_001",
        "input": "æ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ",
        "expected": "å·´é»",
        "category": "åœ°ç†çŸ¥è¯†"
    },
    {
        "id": "test_002",
        "input": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "expected_keywords": ["ç®—æ³•", "æ•°æ®", "æ¨¡å‹", "é¢„æµ‹"],
        "category": "æŠ€æœ¯æ¦‚å¿µ"
    },
    {
        "id": "test_003",
        "input": "å†™ä¸€ä¸ªå…³äºæ˜¥å¤©çš„ä¿³å¥",
        "evaluation_criteria": "åˆ›é€ æ€§ã€ç¬¦åˆä¿³å¥æ ¼å¼",
        "category": "åˆ›æ„å†™ä½œ"
    }
]
```

### Step 2: å®ç°è¯„ä¼°å‡½æ•°

```python
def evaluate_response(test_case, model_output):
    """
    ç®€å•çš„è¯„ä¼°å‡½æ•°ï¼Œæ”¯æŒå¤šç§è¯„ä¼°æ–¹å¼
    """
    score = 0
    feedback = []
    
    # ç²¾ç¡®åŒ¹é…è¯„ä¼°
    if "expected" in test_case:
        if test_case["expected"].lower() in model_output.lower():
            score = 1.0
            feedback.append("âœ… åŒ…å«é¢„æœŸç­”æ¡ˆ")
        else:
            score = 0.0
            feedback.append("âŒ æœªåŒ…å«é¢„æœŸç­”æ¡ˆ")
    
    # å…³é”®è¯è¯„ä¼°
    elif "expected_keywords" in test_case:
        keywords = test_case["expected_keywords"]
        found = sum(1 for kw in keywords if kw in model_output)
        score = found / len(keywords)
        feedback.append(f"ğŸ“ åŒ…å« {found}/{len(keywords)} ä¸ªå…³é”®è¯")
    
    # LLM-as-Judge è¯„ä¼°
    elif "evaluation_criteria" in test_case:
        judge_prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°è¾“å‡ºè´¨é‡ï¼š
        æ ‡å‡†ï¼š{test_case['evaluation_criteria']}
        è¾“å‡ºï¼š{model_output}
        
        è¯·ç»™å‡º0-1çš„è¯„åˆ†ï¼Œå…¶ä¸­1è¡¨ç¤ºå®Œå…¨æ»¡è¶³æ ‡å‡†ï¼š
        """
        
        judge_response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": judge_prompt}],
            temperature=0
        )
        
        try:
            score = float(judge_response.choices[0].message.content.strip())
            feedback.append(f"ğŸ¤– AIè¯„åˆ†: {score}")
        except:
            score = 0.5
            feedback.append("âš ï¸ è¯„åˆ†è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°")
    
    return {
        "score": score,
        "feedback": " | ".join(feedback)
    }
```

### Step 3: è¿è¡Œè¯„ä¼°

```python
def run_evaluation():
    """
    æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹
    """
    results = []
    
    print("ğŸš€ å¼€å§‹è¯„ä¼°...\n")
    
    for test_case in test_cases:
        print(f"ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {test_case['id']}: {test_case['input']}")
        
        # ç”Ÿæˆæ¨¡å‹è¾“å‡º
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": test_case["input"]}],
            temperature=0.7,
            max_tokens=100
        )
        
        model_output = response.choices[0].message.content
        print(f"ğŸ’¬ æ¨¡å‹è¾“å‡º: {model_output[:100]}...")
        
        # è¯„ä¼°è¾“å‡º
        evaluation = evaluate_response(test_case, model_output)
        
        # è®°å½•ç»“æœ
        result = {
            "test_id": test_case["id"],
            "category": test_case["category"],
            "input": test_case["input"],
            "output": model_output,
            "score": evaluation["score"],
            "feedback": evaluation["feedback"],
            "timestamp": datetime.now().isoformat()
        }
        
        results.append(result)
        
        # å‘é€åˆ° Langfuseï¼ˆå¯é€‰ï¼‰
        langfuse.score(
            trace_id=test_case["id"],
            name=f"evaluation_{test_case['category']}",
            value=evaluation["score"],
            comment=evaluation["feedback"]
        )
        
        print(f"ğŸ“Š è¯„åˆ†: {evaluation['score']:.2f}")
        print(f"ğŸ’¡ åé¦ˆ: {evaluation['feedback']}\n")
    
    return results

# æ‰§è¡Œè¯„ä¼°
if __name__ == "__main__":
    results = run_evaluation()
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“ˆ è¯„ä¼°æŠ¥å‘Š")
    print("="*50)
    
    total_score = sum(r["score"] for r in results)
    avg_score = total_score / len(results)
    
    print(f"âœ… æ€»æµ‹è¯•æ¡ˆä¾‹: {len(results)}")
    print(f"ğŸ“Š å¹³å‡å¾—åˆ†: {avg_score:.2%}")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    categories = {}
    for r in results:
        cat = r["category"]
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(r["score"])
    
    print("\nğŸ“‚ åˆ†ç±»ç»Ÿè®¡:")
    for cat, scores in categories.items():
        avg = sum(scores) / len(scores)
        print(f"  - {cat}: {avg:.2%} ({len(scores)} ä¸ªæµ‹è¯•)")
    
    # ä¿å­˜ç»“æœ
    with open("evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° evaluation_results.json")
```

## æŸ¥çœ‹è¯„ä¼°ç»“æœï¼ˆ3åˆ†é’Ÿï¼‰

### è¿è¡Œè¯„ä¼°è„šæœ¬

```bash
python first_evaluation.py
```

### é¢„æœŸè¾“å‡º

```
ğŸš€ å¼€å§‹è¯„ä¼°...

ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ test_001: æ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ
ğŸ’¬ æ¨¡å‹è¾“å‡º: æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»...
ğŸ“Š è¯„åˆ†: 1.00
ğŸ’¡ åé¦ˆ: âœ… åŒ…å«é¢„æœŸç­”æ¡ˆ

ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ test_002: è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ 
ğŸ’¬ æ¨¡å‹è¾“å‡º: æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ...
ğŸ“Š è¯„åˆ†: 0.75
ğŸ’¡ åé¦ˆ: ğŸ“ åŒ…å« 3/4 ä¸ªå…³é”®è¯

ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ test_003: å†™ä¸€ä¸ªå…³äºæ˜¥å¤©çš„ä¿³å¥
ğŸ’¬ æ¨¡å‹è¾“å‡º: æ¨±èŠ±é£˜è½æ—¶ï¼Œå¾®é£è½»æ‹‚è¿‡æ¹–é¢ï¼Œæ˜¥æ„æ­£æµ“æ—¶...
ğŸ“Š è¯„åˆ†: 0.85
ğŸ’¡ åé¦ˆ: ğŸ¤– AIè¯„åˆ†: 0.85

==================================================
ğŸ“ˆ è¯„ä¼°æŠ¥å‘Š
==================================================
âœ… æ€»æµ‹è¯•æ¡ˆä¾‹: 3
ğŸ“Š å¹³å‡å¾—åˆ†: 86.67%

ğŸ“‚ åˆ†ç±»ç»Ÿè®¡:
  - åœ°ç†çŸ¥è¯†: 100.00% (1 ä¸ªæµ‹è¯•)
  - æŠ€æœ¯æ¦‚å¿µ: 75.00% (1 ä¸ªæµ‹è¯•)
  - åˆ›æ„å†™ä½œ: 85.00% (1 ä¸ªæµ‹è¯•)

ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° evaluation_results.json
```

### ç»“æœåˆ†æ

æŸ¥çœ‹ç”Ÿæˆçš„ `evaluation_results.json`ï¼š

```json
[
  {
    "test_id": "test_001",
    "category": "åœ°ç†çŸ¥è¯†",
    "input": "æ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ",
    "output": "æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»ã€‚",
    "score": 1.0,
    "feedback": "âœ… åŒ…å«é¢„æœŸç­”æ¡ˆ",
    "timestamp": "2024-01-20T10:30:45.123456"
  },
  // ... æ›´å¤šç»“æœ
]
```

## è¿›é˜¶ï¼šå¯è§†åŒ–ç»“æœ

### åˆ›å»ºç®€å•çš„å¯è§†åŒ–è„šæœ¬

```python
# visualize_results.py
import json
import pandas as pd
import matplotlib.pyplot as plt

# åŠ è½½ç»“æœ
with open("evaluation_results.json", "r", encoding="utf-8") as f:
    results = json.load(f)

# è½¬æ¢ä¸º DataFrame
df = pd.DataFrame(results)

# åˆ›å»ºå¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# åˆ†ç±»å¾—åˆ†æŸ±çŠ¶å›¾
category_scores = df.groupby('category')['score'].mean()
axes[0].bar(category_scores.index, category_scores.values)
axes[0].set_title('å„ç±»åˆ«å¹³å‡å¾—åˆ†')
axes[0].set_ylabel('å¾—åˆ†')
axes[0].set_ylim(0, 1)

# å¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾
axes[1].hist(df['score'], bins=10, edgecolor='black')
axes[1].set_title('å¾—åˆ†åˆ†å¸ƒ')
axes[1].set_xlabel('å¾—åˆ†')
axes[1].set_ylabel('é¢‘æ¬¡')

plt.tight_layout()
plt.savefig('evaluation_visualization.png')
plt.show()

print("ğŸ“Š å¯è§†åŒ–å·²ä¿å­˜åˆ° evaluation_visualization.png")
```

## å¸¸è§é—®é¢˜è§£å†³

<Callout type="warning">
**API è°ƒç”¨å¤±è´¥ï¼Ÿ**
- æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®
- ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸
- æŸ¥çœ‹ API é…é¢æ˜¯å¦å……è¶³
</Callout>

<Tabs items={['é”™è¯¯1: APIå¯†é’¥', 'é”™è¯¯2: ä¾èµ–åŒ…', 'é”™è¯¯3: ç¼–ç ']}>
  <Tab value="é”™è¯¯1: APIå¯†é’¥">
    ```python
    # é—®é¢˜ï¼šopenai.error.AuthenticationError
    # è§£å†³æ–¹æ¡ˆï¼š
    import os
    
    # æ–¹æ³•1ï¼šç›´æ¥è®¾ç½®
    openai.api_key = "sk-your-actual-key"
    
    # æ–¹æ³•2ï¼šä»ç¯å¢ƒå˜é‡è¯»å–
    openai.api_key = os.getenv("OPENAI_API_KEY")
    
    # æ–¹æ³•3ï¼šä½¿ç”¨ python-dotenv
    from dotenv import load_dotenv
    load_dotenv()
    ```
  </Tab>
  
  <Tab value="é”™è¯¯2: ä¾èµ–åŒ…">
    ```bash
    # é—®é¢˜ï¼šModuleNotFoundError
    # è§£å†³æ–¹æ¡ˆï¼š
    
    # æ£€æŸ¥å·²å®‰è£…çš„åŒ…
    pip list
    
    # é‡æ–°å®‰è£…æ‰€æœ‰ä¾èµ–
    pip install -r requirements.txt
    
    # æˆ–è€…å•ç‹¬å®‰è£…ç¼ºå¤±çš„åŒ…
    pip install openai langfuse
    ```
  </Tab>
  
  <Tab value="é”™è¯¯3: ç¼–ç ">
    ```python
    # é—®é¢˜ï¼šUnicodeDecodeError
    # è§£å†³æ–¹æ¡ˆï¼š
    
    # è¯»å–æ–‡ä»¶æ—¶æŒ‡å®šç¼–ç 
    with open("file.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # å†™å…¥æ–‡ä»¶æ—¶ç¡®ä¿æ­£ç¡®ç¼–ç 
    with open("output.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False)
    ```
  </Tab>
</Tabs>

## ä¸‹ä¸€æ­¥å»ºè®®

æ­å–œä½ å®Œæˆäº†ç¬¬ä¸€ä¸ª LLM è¯„ä¼°ï¼ğŸ‰ ç°åœ¨ä½ å¯ä»¥ï¼š

<Cards>
  <Card title="æ‰©å±•æµ‹è¯•é›†" description="æ·»åŠ æ›´å¤šæµ‹è¯•æ¡ˆä¾‹ï¼Œè¦†ç›–æ›´å¤šåœºæ™¯">
    ```python
    # æ·»åŠ æ›´å¤šæµ‹è¯•ç±»å‹
    test_cases.extend([
        {"id": "test_004", "input": "...", ...},
        {"id": "test_005", "input": "...", ...}
    ])
    ```
  </Card>
  
  <Card title="ä¼˜åŒ–è¯„ä¼°å‡½æ•°" description="å®ç°æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘">
    ```python
    # æ·»åŠ è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°
    from sentence_transformers import SentenceTransformer
    
    def semantic_similarity(text1, text2):
        # å®ç°è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
        pass
    ```
  </Card>
  
  <Card title="é›†æˆåˆ° CI/CD" description="è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹">
    ```yaml
    # .github/workflows/evaluate.yml
    on: [push]
    jobs:
      evaluate:
        runs-on: ubuntu-latest
        steps:
          - run: python first_evaluation.py
    ```
  </Card>
</Cards>

## æ ¸å¿ƒè¦ç‚¹å›é¡¾

- âœ… è¯„ä¼°å¯ä»¥å¾ˆç®€å•ï¼Œä»å°è§„æ¨¡å¼€å§‹
- âœ… é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•ï¼ˆç²¾ç¡®åŒ¹é…ã€å…³é”®è¯ã€LLM-as-Judgeï¼‰
- âœ… è®°å½•å’Œåˆ†æç»“æœï¼ŒæŒç»­æ”¹è¿›
- âœ… å¯è§†åŒ–å¸®åŠ©æ›´å¥½åœ°ç†è§£ç»“æœ

## ç»§ç»­å­¦ä¹ 

<Card title="æ ¸å¿ƒæ¦‚å¿µ â†’" href="/docs/getting-started/concepts" description="æ·±å…¥ç†è§£è¯„ä¼°çš„æ ¸å¿ƒæ¦‚å¿µå’Œæœ¯è¯­" />