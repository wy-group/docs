# å¤§æ¨¡å‹è¯„ä¼°æŒ‡å—

## æ¦‚è¿°

å¤§æ¨¡å‹è¯„ä¼°æ˜¯ç¡®ä¿ LLM åº”ç”¨è´¨é‡å’Œæ€§èƒ½çš„å…³é”®ç¯èŠ‚ã€‚æœ¬æŒ‡å—åŸºäº Langfuse çš„æœ€ä½³å®è·µï¼Œä»‹ç»å¦‚ä½•ç³»ç»Ÿåœ°è¯„ä¼°å¤§æ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€ç›‘æ§ç”Ÿäº§ç¯å¢ƒå¥åº·åº¦ï¼Œä»¥åŠåœ¨å¼€å‘è¿‡ç¨‹ä¸­æµ‹è¯•å˜æ›´ã€‚

## è¯„ä¼°æ–¹æ³•

### 1. LLM-as-a-Judge (LLM ä½œä¸ºè¯„åˆ¤è€…)

ä½¿ç”¨å¦ä¸€ä¸ªå¤§æ¨¡å‹æ¥è¯„ä¼°ç›®æ ‡æ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚

**ä¼˜åŠ¿ï¼š**
- è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ï¼Œå¯å¤§è§„æ¨¡æ‰§è¡Œ
- è¯„ä¼°æ ‡å‡†ä¸€è‡´æ€§å¼º
- æˆæœ¬ç›¸å¯¹è¾ƒä½

**å®æ–½æ­¥éª¤ï¼š**
```python
# ä½¿ç”¨è¯„åˆ¤ LLM è¯„ä¼°è¾“å‡º
def llm_judge_evaluation(output, criteria):
    judge_prompt = f"""
    è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°è¾“å‡ºè´¨é‡ï¼š
    æ ‡å‡†ï¼š{criteria}
    è¾“å‡ºï¼š{output}
    
    è¯„åˆ†ï¼ˆ0-1ï¼‰ï¼š
    """
    score = judge_llm.generate(judge_prompt)
    return score
```

**è¯„ä¼°ç»´åº¦ï¼š**
- å‡†ç¡®æ€§ (Accuracy)
- ç›¸å…³æ€§ (Relevance)
- å®Œæ•´æ€§ (Completeness)
- è¯­è¨€æµç•…åº¦ (Fluency)
- å®‰å…¨æ€§ (Safety)

### 2. äººå·¥æ ‡æ³¨ (Manual Annotations)

é€šè¿‡äººå·¥å®¡æ ¸å»ºç«‹è¯„ä¼°åŸºå‡†ã€‚

**å®æ–½æµç¨‹ï¼š**
1. åˆ›å»ºæ ‡æ³¨é˜Ÿåˆ—
2. åˆ†é…æ ‡æ³¨ä»»åŠ¡ç»™è¯„å®¡å‘˜
3. æ”¶é›†å¹¶æ±‡æ€»æ ‡æ³¨ç»“æœ
4. è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡ï¼ˆå¦‚ Cohen's Kappaï¼‰

**æ ‡æ³¨ç•Œé¢ç¤ºä¾‹ï¼š**
```json
{
  "trace_id": "abc123",
  "output": "æ¨¡å‹ç”Ÿæˆçš„å“åº”",
  "annotation_fields": {
    "quality": ["ä¼˜ç§€", "è‰¯å¥½", "ä¸€èˆ¬", "å·®"],
    "relevance": [1, 2, 3, 4, 5],
    "contains_errors": ["æ˜¯", "å¦"]
  }
}
```

### 3. ç”¨æˆ·åé¦ˆ (User Feedback)

æ”¶é›†ç»ˆç«¯ç”¨æˆ·çš„ç›´æ¥åé¦ˆã€‚

**åé¦ˆæ”¶é›†æ–¹å¼ï¼š**
- ğŸ‘/ğŸ‘ ç®€å•åé¦ˆ
- æ˜Ÿçº§è¯„åˆ†ï¼ˆ1-5 æ˜Ÿï¼‰
- è¯¦ç»†åé¦ˆè¡¨å•

**å®ç°ç¤ºä¾‹ï¼š**
```python
# æ”¶é›†ç”¨æˆ·åé¦ˆ
langfuse.score(
    trace_id="user_session_123",
    name="user_satisfaction",
    value=4,  # 1-5 æ˜Ÿè¯„åˆ†
    comment="å“åº”å¾ˆæœ‰å¸®åŠ©ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢"
)
```

### 4. è‡ªå®šä¹‰è¯„åˆ† (Custom Scoring)

æ ¹æ®ä¸šåŠ¡éœ€æ±‚å®šåˆ¶è¯„ä¼°æŒ‡æ ‡ã€‚

**å¸¸è§è‡ªå®šä¹‰æŒ‡æ ‡ï¼š**
```python
# å“åº”æ—¶é—´è¯„ä¼°
def evaluate_latency(trace):
    latency_ms = trace.end_time - trace.start_time
    if latency_ms < 1000:
        return 1.0  # ä¼˜ç§€
    elif latency_ms < 3000:
        return 0.7  # è‰¯å¥½
    else:
        return 0.3  # éœ€æ”¹è¿›

# Token ä½¿ç”¨æ•ˆç‡
def evaluate_token_efficiency(trace):
    token_count = trace.usage.total_tokens
    output_quality = trace.scores.get("quality", 0)
    efficiency = output_quality / (token_count / 1000)
    return min(efficiency, 1.0)
```

## è¯„ä¼°æŒ‡æ ‡ä½“ç³»

### æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ç±»å‹ | æŒ‡æ ‡åç§° | è®¡ç®—æ–¹æ³• | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| **è´¨é‡æŒ‡æ ‡** | å‡†ç¡®ç‡ | æ­£ç¡®å“åº”æ•° / æ€»å“åº”æ•° | äº‹å®æ€§é—®ç­” |
| | BLEU åˆ†æ•° | N-gram é‡å åº¦ | ç¿»è¯‘ä»»åŠ¡ |
| | ROUGE åˆ†æ•° | å¬å›ç‡å¯¼å‘çš„è¯„ä¼° | æ‘˜è¦ä»»åŠ¡ |
| **æ€§èƒ½æŒ‡æ ‡** | å“åº”å»¶è¿Ÿ | P50, P90, P99 å»¶è¿Ÿ | å®æ—¶åº”ç”¨ |
| | ååé‡ | è¯·æ±‚æ•° / ç§’ | é«˜å¹¶å‘åœºæ™¯ |
| | Token æ•ˆç‡ | è¾“å‡ºè´¨é‡ / Token æ¶ˆè€— | æˆæœ¬ä¼˜åŒ– |
| **ä¸šåŠ¡æŒ‡æ ‡** | ç”¨æˆ·æ»¡æ„åº¦ | å¹³å‡è¯„åˆ† | äº§å“ä½“éªŒ |
| | ä»»åŠ¡å®Œæˆç‡ | æˆåŠŸå®Œæˆæ•° / æ€»ä»»åŠ¡æ•° | ä»»åŠ¡å‹å¯¹è¯ |
| | è½¬åŒ–ç‡ | è¾¾æˆç›®æ ‡æ•° / æ€»ä¼šè¯æ•° | å•†ä¸šåº”ç”¨ |

### è¯„åˆ†ç±»å‹

```python
# æ•°å€¼å‹è¯„åˆ† (Numeric)
langfuse.score(
    trace_id="123",
    name="accuracy",
    value=0.85,  # 0-1 èŒƒå›´
    data_type="NUMERIC"
)

# å¸ƒå°”å‹è¯„åˆ† (Boolean)
langfuse.score(
    trace_id="123",
    name="contains_hallucination",
    value=False,
    data_type="BOOLEAN"
)

# åˆ†ç±»å‹è¯„åˆ† (Categorical)
langfuse.score(
    trace_id="123",
    name="sentiment",
    value="positive",  # positive/neutral/negative
    data_type="CATEGORICAL"
)
```

## è¯„ä¼°æµç¨‹

### 1. å¼€å‘é˜¶æ®µè¯„ä¼°

```mermaid
graph LR
    A[åˆ›å»ºæµ‹è¯•æ•°æ®é›†] --> B[è¿è¡ŒåŸºå‡†æµ‹è¯•]
    B --> C[è¿­ä»£ä¼˜åŒ–]
    C --> D[å¯¹æ¯”è¯„ä¼°]
    D --> E[é€‰æ‹©æœ€ä½³ç‰ˆæœ¬]
```

**å®æ–½æ­¥éª¤ï¼š**
1. **å‡†å¤‡æ•°æ®é›†**
   ```python
   test_dataset = [
       {"input": "é—®é¢˜1", "expected": "é¢„æœŸç­”æ¡ˆ1"},
       {"input": "é—®é¢˜2", "expected": "é¢„æœŸç­”æ¡ˆ2"},
       # ...
   ]
   ```

2. **è¿è¡Œè¯„ä¼°**
   ```python
   for test_case in test_dataset:
       output = model.generate(test_case["input"])
       score = evaluate(output, test_case["expected"])
       langfuse.score(
           name="dev_evaluation",
           value=score
       )
   ```

3. **A/B æµ‹è¯•**
   ```python
   # å¯¹æ¯”ä¸åŒ prompt æˆ–æ¨¡å‹ç‰ˆæœ¬
   results_a = evaluate_version("v1")
   results_b = evaluate_version("v2")
   
   if results_b.mean_score > results_a.mean_score:
       deploy_version("v2")
   ```

### 2. ç”Ÿäº§ç¯å¢ƒç›‘æ§

**å®æ—¶ç›‘æ§æŒ‡æ ‡ï¼š**
```python
# è®¾ç½®ç›‘æ§å‘Šè­¦
monitoring_config = {
    "accuracy_threshold": 0.8,
    "latency_p99_ms": 3000,
    "error_rate": 0.01
}

# å®æ—¶è¯„ä¼°
@monitor
def production_inference(input):
    start_time = time.time()
    try:
        output = model.generate(input)
        latency = time.time() - start_time
        
        # å¼‚æ­¥è¯„ä¼°è´¨é‡
        async_evaluate(output)
        
        # è®°å½•æŒ‡æ ‡
        langfuse.score(
            name="production_latency",
            value=latency
        )
        
        return output
    except Exception as e:
        log_error(e)
        raise
```

### 3. æŒç»­æ”¹è¿›å¾ªç¯

```mermaid
graph TD
    A[æ”¶é›†ç”Ÿäº§æ•°æ®] --> B[è¯†åˆ«é—®é¢˜æ¡ˆä¾‹]
    B --> C[æ·»åŠ åˆ°æµ‹è¯•é›†]
    C --> D[æ”¹è¿›æ¨¡å‹/Prompt]
    D --> E[è¯„ä¼°æ”¹è¿›æ•ˆæœ]
    E --> F{è¾¾æ ‡?}
    F -->|æ˜¯| G[éƒ¨ç½²æ›´æ–°]
    F -->|å¦| D
    G --> A
```

## è¯„ä¼°æ•°æ®ç®¡ç†

### æ•°æ®é›†ç»„ç»‡

```python
# åˆ›å»ºè¯„ä¼°æ•°æ®é›†
dataset = langfuse.create_dataset(
    name="customer_service_qa",
    description="å®¢æœé—®ç­”è¯„ä¼°é›†",
    metadata={
        "version": "1.0",
        "domain": "customer_service",
        "size": 1000
    }
)

# æ·»åŠ æµ‹è¯•æ¡ˆä¾‹
dataset.create_item(
    input="å¦‚ä½•é€€è´§ï¼Ÿ",
    expected_output="é€€è´§æµç¨‹è¯´æ˜...",
    metadata={
        "category": "returns",
        "difficulty": "easy"
    }
)
```

### ç‰ˆæœ¬ç®¡ç†

```python
# è·Ÿè¸ªä¸åŒç‰ˆæœ¬çš„è¯„ä¼°ç»“æœ
evaluation_run = langfuse.create_run(
    name="eval_2024_01",
    dataset_id=dataset.id,
    model_version="gpt-4-turbo",
    prompt_version="v2.3"
)

# è®°å½•è¯„ä¼°ç»“æœ
for item in dataset.items:
    output = model.generate(item.input)
    score = evaluate(output, item.expected_output)
    
    evaluation_run.add_observation(
        item_id=item.id,
        output=output,
        scores={"accuracy": score}
    )
```

## è¯„ä¼°ä»ªè¡¨æ¿

### å¯è§†åŒ–é…ç½®

```python
# é…ç½®è¯„ä¼°ä»ªè¡¨æ¿
dashboard_config = {
    "metrics": [
        {
            "name": "accuracy",
            "type": "line_chart",
            "aggregation": "mean",
            "time_window": "1h"
        },
        {
            "name": "latency",
            "type": "histogram",
            "percentiles": [50, 90, 99]
        },
        {
            "name": "user_satisfaction",
            "type": "gauge",
            "threshold": 4.0
        }
    ],
    "filters": {
        "model_version": ["v1", "v2"],
        "environment": ["dev", "staging", "production"]
    }
}
```

### æŠ¥å‘Šç”Ÿæˆ

```python
def generate_evaluation_report(run_id):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    run = langfuse.get_run(run_id)
    
    report = {
        "summary": {
            "total_cases": len(run.observations),
            "mean_accuracy": run.scores.accuracy.mean(),
            "median_latency": run.latency.median(),
            "success_rate": run.success_count / run.total_count
        },
        "detailed_metrics": {
            "by_category": analyze_by_category(run),
            "error_analysis": analyze_errors(run),
            "performance_breakdown": analyze_performance(run)
        },
        "recommendations": generate_recommendations(run)
    }
    
    return report
```

## æœ€ä½³å®è·µ

### 1. å»ºç«‹è¯„ä¼°åŸºå‡†

- **é»„é‡‘æ•°æ®é›†**ï¼šç»´æŠ¤é«˜è´¨é‡çš„æµ‹è¯•é›†ï¼Œè¦†ç›–å„ç§åœºæ™¯
- **äººå·¥åŸºå‡†**ï¼šå®šæœŸè¿›è¡Œäººå·¥è¯„ä¼°ï¼Œå»ºç«‹è´¨é‡åŸºå‡†çº¿
- **ç«å“å¯¹æ¯”**ï¼šä¸å…¶ä»–æ¨¡å‹æˆ–è§£å†³æ–¹æ¡ˆè¿›è¡Œå¯¹æ¯”è¯„ä¼°

### 2. å¤šç»´åº¦è¯„ä¼°

```python
# ç»¼åˆè¯„åˆ†ç³»ç»Ÿ
def calculate_composite_score(trace):
    weights = {
        "accuracy": 0.4,
        "relevance": 0.3,
        "latency": 0.2,
        "cost_efficiency": 0.1
    }
    
    scores = {}
    scores["accuracy"] = evaluate_accuracy(trace)
    scores["relevance"] = evaluate_relevance(trace)
    scores["latency"] = evaluate_latency(trace)
    scores["cost_efficiency"] = evaluate_cost(trace)
    
    composite = sum(scores[k] * weights[k] for k in weights)
    
    return {
        "composite_score": composite,
        "breakdown": scores
    }
```

### 3. è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

```python
# CI/CD é›†æˆ
def automated_evaluation_pipeline():
    """è‡ªåŠ¨åŒ–è¯„ä¼°ç®¡é“"""
    
    # 1. æ‹‰å–æœ€æ–°ä»£ç 
    checkout_latest_code()
    
    # 2. è¿è¡Œå•å…ƒæµ‹è¯•
    run_unit_tests()
    
    # 3. è¿è¡Œè¯„ä¼°å¥—ä»¶
    results = run_evaluation_suite()
    
    # 4. æ£€æŸ¥è´¨é‡é—¨æ§›
    if not meets_quality_threshold(results):
        raise QualityCheckFailed(results)
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    report = generate_report(results)
    
    # 6. é€šçŸ¥ç›¸å…³äººå‘˜
    notify_stakeholders(report)
    
    return results
```

### 4. å¼‚å¸¸æ£€æµ‹

```python
# æ£€æµ‹å¼‚å¸¸è¡Œä¸º
def detect_anomalies(trace):
    anomalies = []
    
    # æ£€æµ‹å¹»è§‰
    if contains_hallucination(trace.output):
        anomalies.append({
            "type": "hallucination",
            "severity": "high",
            "details": extract_hallucination_details(trace)
        })
    
    # æ£€æµ‹æœ‰å®³å†…å®¹
    if contains_harmful_content(trace.output):
        anomalies.append({
            "type": "harmful_content",
            "severity": "critical",
            "details": extract_harmful_content(trace)
        })
    
    # æ£€æµ‹æ€§èƒ½å¼‚å¸¸
    if trace.latency > LATENCY_THRESHOLD:
        anomalies.append({
            "type": "performance",
            "severity": "medium",
            "details": {"latency_ms": trace.latency}
        })
    
    return anomalies
```

## å·¥å…·é›†æˆ

### Langfuse SDK é›†æˆ

```python
from langfuse import Langfuse

# åˆå§‹åŒ–å®¢æˆ·ç«¯
langfuse = Langfuse(
    public_key="your_public_key",
    secret_key="your_secret_key",
    host="https://cloud.langfuse.com"
)

# è£…é¥°å™¨æ¨¡å¼
@observe()
def generate_response(prompt):
    response = model.generate(prompt)
    
    # è‡ªåŠ¨è®°å½•è¿½è¸ªä¿¡æ¯
    langfuse.score(
        name="quality",
        value=evaluate_quality(response)
    )
    
    return response
```

### API é›†æˆ

```python
import requests

# é€šè¿‡ API æäº¤è¯„åˆ†
def submit_score_via_api(trace_id, score_name, score_value):
    url = "https://api.langfuse.com/scores"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "traceId": trace_id,
        "name": score_name,
        "value": score_value,
        "timestamp": datetime.now().isoformat()
    }
    
    response = requests.post(url, headers=headers, json=data)
    return response.json()
```

## æ€»ç»“

å¤§æ¨¡å‹è¯„ä¼°æ˜¯ä¸€ä¸ªæŒç»­è¿­ä»£çš„è¿‡ç¨‹ï¼Œéœ€è¦ç»“åˆå¤šç§è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡æ¥å…¨é¢è¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡ Langfuse ç­‰å·¥å…·çš„æ”¯æŒï¼Œå¯ä»¥å»ºç«‹ç³»ç»ŸåŒ–çš„è¯„ä¼°ä½“ç³»ï¼Œå®ç°ï¼š

1. **å¼€å‘é˜¶æ®µ**ï¼šå¿«é€Ÿè¿­ä»£å’Œä¼˜åŒ–
2. **æµ‹è¯•é˜¶æ®µ**ï¼šå…¨é¢éªŒè¯è´¨é‡
3. **ç”Ÿäº§ç¯å¢ƒ**ï¼šå®æ—¶ç›‘æ§å’Œå‘Šè­¦
4. **æŒç»­æ”¹è¿›**ï¼šåŸºäºæ•°æ®é©±åŠ¨çš„ä¼˜åŒ–

å…³é”®æˆåŠŸå› ç´ ï¼š
- âœ… å»ºç«‹å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»
- âœ… è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹
- âœ… æŒç»­æ”¶é›†å’Œåˆ†æåé¦ˆ
- âœ… å¿«é€Ÿå“åº”å’Œè¿­ä»£æ”¹è¿›
- âœ… è·¨å›¢é˜Ÿåä½œå’ŒçŸ¥è¯†å…±äº«

é€šè¿‡éµå¾ªæœ¬æŒ‡å—çš„æœ€ä½³å®è·µï¼Œå¯ä»¥ç¡®ä¿å¤§æ¨¡å‹åº”ç”¨å§‹ç»ˆä¿æŒé«˜è´¨é‡å’Œå¯é æ€§ã€‚